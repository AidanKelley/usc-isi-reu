@prefix schema: <https://schema.org/> .
@prefix sd: <https://w3id.org/okn/o/sd#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

<https://example.org/objects/Software/JimmySuen/integral-human-pose/> a sd:Software ;
    sd:author <https://example.org/objects/Person/JimmySuen> ;
    sd:citation """@article{sun2017integral,
  title={Integral human pose regression},
  author={Sun, Xiao and Xiao, Bin and Liang, Shuang and Wei, Yichen},
  journal={arXiv preprint arXiv:1711.08229},
  year={2017}
}"""^^xsd:string,
        """@article{sun2018integral,
  title={An Integral Pose Regression System for the ECCV2018 PoseTrack Challenge},
  author={Sun, Xiao and Li, Chuankang and Lin, Stephen},
  journal={arXiv preprint arXiv:1809.06079},
  year={2018}
}"""^^xsd:string,
        """If you find Integral Regression useful in your research, please consider citing:
```
@article{sun2017integral,
  title={Integral human pose regression},
  author={Sun, Xiao and Xiao, Bin and Liang, Shuang and Wei, Yichen},
  journal={arXiv preprint arXiv:1711.08229},
  year={2017}
}
```
```
@article{sun2018integral,
  title={An Integral Pose Regression System for the ECCV2018 PoseTrack Challenge},
  author={Sun, Xiao and Li, Chuankang and Lin, Stephen},
  journal={arXiv preprint arXiv:1809.06079},
  year={2018}
}
```

"""^^xsd:string ;
    sd:dateCreated "2018-07-20T03:15:18+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T04:09:05+00:00"^^xsd:dateTime ;
    sd:description """**Integral Regression** is initially described in an [ECCV 2018 paper](https://arxiv.org/abs/1711.08229). ([Slides](https://jimmysuen.github.io/slides/xiaosun_integral_human_pose_regression.pptx)).

We build a [3D pose estimation system](https://arxiv.org/abs/1809.06079) based mainly on the Integral Regression, placing second in the [ECCV2018 3D Human Pose Estimation Challenge](http://vision.imar.ro/human3.6m/ranking.php). Note that, the winner [Sarandi et al.](https://arxiv.org/pdf/1809.04987.pdf) also uses the Integral Regression (or soft-argmax) with a better [augmented 3D dataset](https://github.com/isarandi/synthetic-occlusion) in their method indicating the Integral Regression is the currently state-of-the-art 3D human pose estimation method.

The Integral Regression is also known as soft-argmax. Please refer to two contemporary works ([Luvizon et al.](https://arxiv.org/abs/1710.02322) and [Nibali et al.](https://arxiv.org/abs/1801.07372)) for a better comparision and more comprehensive understanding.





"""^^xsd:string,
        "Integral Human Pose Regression"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/JimmySuen/integral-human-pose/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """1. Download Human3.6M(ECCV18 Challenge) image from [Human3.6M Dataset](http://vision.imar.ro/human3.6m/description.php) and our processed annotation from [Baidu Disk](https://pan.baidu.com/s/1Qg4dH8PBXm8SzApI-uu0GA) (code: kfsm) or [Google Drive](https://drive.google.com/file/d/1wZynXUq91yECVRTFV8Tetvo271BXzxwI/view?usp=sharing)
2. Download MPII image from [MPII Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/)
3. Download COCO2017 image from [COCO Dataset](http://cocodataset.org/#home)
4. Download cache file from [Dropbox](https://www.dropbox.com/sh/uouev0a1ao84ofd/AADAjJUdr_Fm-eubk7c_s2JTa?dl=0)
5. Organize data like this
```
${PROJECT_ROOT}
 `-- data
     `-- coco
        |-- images
        |-- annotations
        |-- COCO_train2017_cache
     `-- mpii
        |-- images
        |-- annot
        |-- mpii_train_cache
        |-- mpii_valid_cache
     `-- hm36
        |-- images
        |-- annot
        |-- HM36_train_cache
        |-- HM36_validmin_cache
     `-- hm36_eccv_challenge
        `-- Train
            |-- IMG
            |-- POSE
        `-- Val
            |-- IMG
            |-- POSE
        `-- Test
            |-- IMG
        |-- HM36_eccv_challenge_Train_cache
        |-- HM36_eccv_challenge_Test_cache
        |-- HM36_eccv_challenge_Val_cache
```

#:#: Usage
We have placed some example config files in *experiments* folder, and you can use them straight forward. Don't modify them unless you know exactly what it means.
#:#:#: Train 
For [Integral Human Pose Regression](https://arxiv.org/abs/1711.08229), cd to *pytorch_projects/integral_human_pose* 
**Integral Regression**
```bash
python train.py --cfg=experiments/hm36/resnet50v1_ft/d-mh_ps-256_deconv256x3_min-int-l1_adam_bs32-4gpus_x300-270-290/lr1e-3.yaml --dataroot=../../data/  
```
**Direct Joint Regression**
```bash
python train.py --cfg=experiments/hm36/resnet50v1_ft/d-mh_ps-256_dj_l1_adam_bs32-4gpus_x140-90-120/lr1e-3.yaml --dataroot=../../data/
```

For [3D pose estimation system](https://arxiv.org/abs/1809.06079) of ECCV18 Challenge, cd to *pytorch_projects/hm36_challenge*
```bash
python train.py --cfg=experiments/hm36/resnet152v1_ft/d-mh_ps-256_deconv256x3_min-int-l1_adam_bs24-4gpus_x300-270-290/lr1e-3.yaml --dataroot=../../data/
```

By default, logging and model will be saved to *log* and *output* folder respectively.

#:#:#: Test
To run evaluation on CHALL_H80K Val dataset
1. Download [model](https://www.dropbox.com/s/hfz5nkd39uisvbr/model_chall_train_152ft_384x288.pth.tar?dl=0)
2. Place it under $project_root/model/hm36_challenge
3. cd to *$project_root/pytorch_projects/hm36_challenge*
4. execute command below
```bash
python test.py --cfg experiments/hm36/resnet152v1_ft/d-mch_384x288_deconv256x3_min-int-l1_adam_bs12-4gpus/lr1e-4_x300-270-290.yaml --model=../../model/hm36_challenge/model_chall_train_152ft_384x288.pth.tar
```
"""^^xsd:string,
        """We recommend installing python from [Anaconda](https://www.anaconda.com/), installing pytorch following guide on [PyTorch](https://pytorch.org/) according to your specific CUDA & python version.
In addition, you need to install dependencies below.
```
pip install scipy
pip install matplotlib
pip install opencv-python
pip install easydict
pip install pyyaml
``` 


"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/JimmySuen/integral-human-pose/> ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "JimmySuen/integral-human-pose"^^xsd:string .

<https://example.org/objects/Software/JuliaGeo/LibGEOS.jl/> a sd:Software ;
    sd:author <https://example.org/objects/Person/JuliaGeo> ;
    sd:dateCreated "2015-01-30T19:30:32+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-26T21:29:29+00:00"^^xsd:dateTime ;
    sd:description "Julia package for manipulation and analysis of planar geometric objects"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/JuliaGeo/LibGEOS.jl/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/JuliaGeo/LibGEOS.jl/> ;
    sd:keywords "computational-geometry"^^xsd:string,
        "geo"^^xsd:string,
        "geometry"^^xsd:string,
        "geos"^^xsd:string,
        "geospatial"^^xsd:string,
        "gis"^^xsd:string,
        "julia"^^xsd:string,
        "vector"^^xsd:string ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "JuliaGeo/LibGEOS.jl"^^xsd:string .

<https://example.org/objects/Software/LMescheder/GAN_stability/> a sd:Software ;
    sd:author <https://example.org/objects/Person/LMescheder> ;
    sd:citation """  year = {2018} 
"""^^xsd:string,
        """@INPROCEEDINGS{Mescheder2018ICML,
  author = {Lars Mescheder and Sebastian Nowozin and Andreas Geiger},
  title = {Which Training Methods for GANs do actually Converge?},
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2018}
}"""^^xsd:string ;
    sd:dateCreated "2018-07-02T09:38:25+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-02T09:59:32+00:00"^^xsd:dateTime ;
    sd:description "Code for paper \"Which Training Methods for GANs do actually Converge? (ICML 2018)\""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/LMescheder/GAN_stability/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/LMescheder/GAN_stability/> ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "LMescheder/GAN_stability"^^xsd:string .

<https://example.org/objects/Software/NSGeophysics/GPRPy/> a sd:Software ;
    sd:author <https://example.org/objects/Person/NSGeophysics> ;
    sd:dateCreated "2018-04-04T01:21:07+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-30T05:02:33+00:00"^^xsd:dateTime ;
    sd:description "Ground Penetrating Radar processing and visualization software for python"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/NSGeophysics/GPRPy/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """**In the following instructions, if you use Windows, use the comands `python` and `pip`. If you use Mac or Linux, use the commands `python3` and `pip3` instead.**

1) Download the GPRPy software from 
   [https://github.com/NSGeophysics/GPRPy/archive/master.zip](https://github.com/NSGeophysics/GPRPy/archive/master.zip). 
   Save the file somewhere on your computer and extract the zip folder. 
   As an **alternative**, you can install git from [https://git-scm.com/](https://git-scm.com/), then run in a command prompt:
   `git clone https://github.com/NSGeophysics/GPRPy.git`
   The advantage of the latter is that you can easily update your software by running from the GPRPy folder in a command prompt:
   `git pull origin master`

2) Install Python 3.7 for example from [https://conda.io/miniconda.html](https://conda.io/miniconda.html)

3) Once the installation finished, open a command prompt that can run Python 
   On Windows: click on Start, then enter "Anaconda Prompt", without the quotation marks into the "Search programs and files" field. On Mac or Linux, open the regular terminal.

4) In the command prompt, change to the directory  where you downloaded the GPRPy files.
   This is usually through a command like for example
   `cd Desktop\\GPRPy`
   if you downloaded GPRPy directly onto your desktop. Then type the following and press enter afterward:
   `python installMigration.py`
   Then type the following and press enter afterward:
   `pip install .`
   **don't forget the period "." at the end of the `pip install` command**


"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/NSGeophysics/GPRPy/> ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "NSGeophysics/GPRPy"^^xsd:string .

<https://example.org/objects/Software/NVIDIA/vid2vid/> a sd:Software ;
    sd:author <https://example.org/objects/Person/NVIDIA> ;
    sd:citation """   year      = {2018}, 
"""^^xsd:string,
        """@inproceedings{wang2018vid2vid,
   author    = {Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu and Guilin Liu
                and Andrew Tao and Jan Kautz and Bryan Catanzaro},
   title     = {Video-to-Video Synthesis},
   booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},   
   year      = {2018},
}"""^^xsd:string ;
    sd:dateCreated "2018-08-14T23:27:15+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T16:22:13+00:00"^^xsd:dateTime ;
    sd:description "Pytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic video-to-video translation."^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/NVIDIA/vid2vid/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """pip install dlib 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/NVIDIA/vid2vid/> ;
    sd:license "None"^^xsd:anyURI ;
    sd:name "NVIDIA/vid2vid"^^xsd:string .

<https://example.org/objects/Software/OpenGeoVis/PVGeo/> a sd:Software ;
    sd:author <https://example.org/objects/Person/OpenGeoVis> ;
    sd:citation """There is a [paper about PVGeo](https://doi.org/10.21105/joss.01451)!

If you are using PVGeo in your scientific research, please help our scientific
visibility by citing our work!

> Sullivan et al., (2019). PVGeo: an open-source Python package for geoscientific visualization in VTK and ParaView. Journal of Open Source Software, 4(38), 1451, https://doi.org/10.21105/joss.01451

See [CITATION.rst](https://github.com/OpenGeoVis/PVGeo/blob/master/CITATION.rst)
for more details.


"""^^xsd:string ;
    sd:dateCreated "2017-07-18T18:05:56+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-19T10:43:24+00:00"^^xsd:dateTime ;
    sd:description "Python package of VTK-based algorithms to analyze geoscientific data and models"^^xsd:string,
        """To begin using the *PVGeo* Python package, create/activate your Python virtual
environment (we highly recommend using anaconda) and install *PVGeo* through pip:

```bash
pip install PVGeo
```

Now *PVGeo* is ready for use in your standard Python environment (2.7 or >=3.6)
with all dependencies installed! Go ahead and test your install:

```bash
python -c "import PVGeo; print(PVGeo.__version__)"
```

Note that Windows users must use Python >=3.6 when outside of ParaView.
Further insight can be found in the [**Getting Started Guide**](http://pvgeo.org/overview/getting-started.html).


"""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/OpenGeoVis/PVGeo/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/OpenGeoVis/PVGeo/> ;
    sd:keywords "3d"^^xsd:string,
        "data-visualization"^^xsd:string,
        "geophysics"^^xsd:string,
        "geosciences"^^xsd:string,
        "model-building"^^xsd:string,
        "open-science"^^xsd:string,
        "paraview"^^xsd:string,
        "paraview-plugin"^^xsd:string,
        "python"^^xsd:string,
        "visual-data-integration"^^xsd:string,
        "visualization"^^xsd:string,
        "vtk"^^xsd:string ;
    sd:license "https://api.github.com/licenses/bsd-3-clause"^^xsd:anyURI ;
    sd:name "OpenGeoVis/PVGeo"^^xsd:string .

<https://example.org/objects/Software/OpenGeoVis/omfvista/> a sd:Software ;
    sd:author <https://example.org/objects/Person/OpenGeoVis> ;
    sd:dateCreated "2019-01-09T20:44:53+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-02T03:13:09+00:00"^^xsd:dateTime ;
    sd:description "3D visualization for the Open Mining Format (omf)"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/OpenGeoVis/omfvista/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """pip install omfvista 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/OpenGeoVis/omfvista/> ;
    sd:license "https://api.github.com/licenses/bsd-3-clause"^^xsd:anyURI ;
    sd:name "OpenGeoVis/omfvista"^^xsd:string .

<https://example.org/objects/Software/OpenGeoscience/geonotebook/> a sd:Software ;
    sd:author <https://example.org/objects/Person/OpenGeoscience> ;
    sd:dateCreated "2016-07-26T19:48:45+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T14:40:15+00:00"^^xsd:dateTime ;
    sd:description "A Jupyter notebook extension for geospatial visualization and analysis"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/OpenGeoscience/geonotebook/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """When developing geonotebook, it is often helpful to install packages as a reference to the
checked out repository rather than copying them to the system `site-packages`.  A "development
install" will allow you to make live changes to python or javascript without reinstalling the
package.
```bash
#: Install the geonotebook python package as "editable"
pip install -e .

#: Install the notebook extension as a symlink
jupyter nbextension install --sys-prefix --symlink --py geonotebook

#: Enable the extension
jupyter serverextension enable --sys-prefix --py geonotebook
jupyter nbextension enable --sys-prefix --py geonotebook

#: Start the javascript builder
cd js
npm run watch
```

"""^^xsd:string,
        """```bash
mkvirtualenv -a . geonotebook

#: Numpy must be fully installed before rasterio
pip install -r prerequirements.txt

pip install -r requirements.txt

pip install .

#: Enable both the notebook and server extensions
jupyter serverextension enable --sys-prefix --py geonotebook
jupyter nbextension enable --sys-prefix --py geonotebook
```

*Note* The `serverextension` and `nbextension` commands accept flags that configure how
and where the extensions are installed.  See `jupyter serverextension --help` for more
information.

"""^^xsd:string,
        """git clone https://github.com/OpenGeoscience/geonotebook.git 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/OpenGeoscience/geonotebook/> ;
    sd:keywords "analysis"^^xsd:string,
        "jupyter"^^xsd:string,
        "jupyter-notebook-extension"^^xsd:string,
        "mapnik"^^xsd:string,
        "notebook"^^xsd:string,
        "opengeoscience"^^xsd:string,
        "python"^^xsd:string,
        "tile-server"^^xsd:string ;
    sd:license "https://api.github.com/licenses/apache-2.0"^^xsd:anyURI ;
    sd:name "OpenGeoscience/geonotebook"^^xsd:string .

<https://example.org/objects/Software/Toblerity/Fiona/> a sd:Software ;
    sd:author <https://example.org/objects/Person/Toblerity> ;
    sd:dateCreated "2011-12-31T19:47:00+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T15:05:09+00:00"^^xsd:dateTime ;
    sd:description "Fiona reads and writes geographic data files"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/Toblerity/Fiona/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """   $ pip install --install-option="-I:\\\\include" --install-option="-lgdal_i" --install-option="-L:\\\\libs" fiona 
"""^^xsd:string,
        """  (fiona_env)$ ./pep-518-install 
"""^^xsd:string,
        """  (fiona_env)$ pip install cython 
  (fiona_env)$ pip install -e .[test] 
"""^^xsd:string,
        """  (fiona_env)$ pip install fiona 
"""^^xsd:string,
        """(fiona_env)$ GDAL_CONFIG=/path/to/gdal-config pip install fiona 
"""^^xsd:string,
        """Or you can use the pep-518-install script:: 
"""^^xsd:string,
        """Python Requirements 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/Toblerity/Fiona/> ;
    sd:keywords "cli"^^xsd:string,
        "cython"^^xsd:string,
        "gdal"^^xsd:string,
        "gis"^^xsd:string,
        "ogr"^^xsd:string,
        "python"^^xsd:string,
        "vector"^^xsd:string ;
    sd:license "None"^^xsd:anyURI ;
    sd:name "Toblerity/Fiona"^^xsd:string .

<https://example.org/objects/Software/Toblerity/Shapely/> a sd:Software ;
    sd:author <https://example.org/objects/Person/Toblerity> ;
    sd:dateCreated "2011-12-31T19:43:11+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T19:30:56+00:00"^^xsd:dateTime ;
    sd:description "Manipulation and analysis of geometric objects"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/Toblerity/Shapely/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """
Shapely may be installed from a source distribution or one of several kinds
of built distribution.

Built distributions
-------------------

Built distributions are the only option for users who do not have or do not
know how to use their platform's compiler and Python SDK, and a good option for
users who would rather not bother.

Linux, OS X, and Windows users can get Shapely wheels with GEOS included from the
Python Package Index with a recent version of pip (8+):

.. code-block:: console

    $ pip install shapely

Shapely is available via system package management tools like apt, yum, and
Homebrew, and is also provided by popular Python distributions like Canopy and
Anaconda. If you use the Conda package manager to install Shapely, be sure to
use the conda-forge channel.

Windows users have another good installation options: the wheels published at
https://www.lfd.uci.edu/~gohlke/pythonlibs/#shapely. These can be installed
using pip by specifying the entire URL.

Source distributions
--------------------

If you want to build Shapely from source for compatibility with other modules
that depend on GEOS (such as cartopy or osgeo.ogr) or want to use a different
version of GEOS than the one included in the project wheels you should first
install the GEOS library, Cython, and Numpy on your system (using apt, yum,
brew, or other means) and then direct pip to ignore the binary wheels.

.. code-block:: console

    $ pip install shapely --no-binary shapely

If you've installed GEOS to a standard location, the geos-config program will
be used to get compiler and linker options. If geos-config is not on your
executable, it can be specified with a GEOS_CONFIG environment variable, e.g.:

.. code-block:: console

    $ GEOS_CONFIG=/path/to/geos-config pip install shapely

"""^^xsd:string,
        """(env)$ pip install -r requirements-dev.txt 
(env)$ pip install -e . 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/Toblerity/Shapely/> ;
    sd:license "None"^^xsd:anyURI ;
    sd:name "Toblerity/Shapely"^^xsd:string .

<https://example.org/objects/Software/XiaLiPKU/RESCAN/> a sd:Software ;
    sd:author <https://example.org/objects/Person/XiaLiPKU> ;
    sd:citation """@inproceedings{li2018recurrent,  
    title={Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining},  
    author={Li, Xia and Wu, Jianlong and Lin, Zhouchen and Liu, Hong and Zha, Hongbin},  
    booktitle={European Conference on Computer Vision},  
    pages={262--277},  
    year={2018},  
    organization={Springer}  
}"""^^xsd:string,
        """If you use our code, please refer this repo.
If you publish your paper that refer to our paper, please cite:

    @inproceedings{li2018recurrent,  
        title={Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining},  
        author={Li, Xia and Wu, Jianlong and Lin, Zhouchen and Liu, Hong and Zha, Hongbin},  
        booktitle={European Conference on Computer Vision},  
        pages={262--277},  
        year={2018},  
        organization={Springer}  
    }

  [1]: https://xialipku.github.io
  [2]: https://jlwu1992.github.io
  [3]: http://cis.pku.edu.cn/faculty/vision/zlin/zlin.htm
  [4]: http://robotics.pkusz.edu.cn/team/leader/
  [5]: http://cis.pku.edu.cn/vision/Visual&Robot/people/zha/
  [6]: ethanlee@pku.edu.cn
  [7]: jlwu1992@pku.edu.cn
  [8]: zlin@pku.edu.cn
  [9]: hongliu@pku.edu.cn
  [10]: http://www.icst.pku.edu.cn/struct/Projects/joint_rain_removal.html
  [11]: https://drive.google.com/drive/folders/0Bw2e6Q0nQQvGbi1xV1Yxd09rY2s
  
"""^^xsd:string ;
    sd:dateCreated "2018-07-06T01:56:12+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-26T09:27:27+00:00"^^xsd:dateTime ;
    sd:description "Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/XiaLiPKU/RESCAN/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/XiaLiPKU/RESCAN/> ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "XiaLiPKU/RESCAN"^^xsd:string .

<https://example.org/objects/Software/ZhouYanzhao/PRM/> a sd:Software ;
    sd:author <https://example.org/objects/Person/ZhouYanzhao> ;
    sd:citation """@INPROCEEDINGS{Zhou2018PRM,
    author = {Zhou, Yanzhao and Zhu, Yi and Ye, Qixiang and Qiu, Qiang and Jiao, Jianbin},
    title = {Weakly Supervised Instance Segmentation using Class Peak Response},
    booktitle = {CVPR},
    year = {2018}
}"""^^xsd:string,
        """If you find the code useful for your research, please cite:
```bibtex
@INPROCEEDINGS{Zhou2018PRM,
    author = {Zhou, Yanzhao and Zhu, Yi and Ye, Qixiang and Qiu, Qiang and Jiao, Jianbin},
    title = {Weakly Supervised Instance Segmentation using Class Peak Response},
    booktitle = {CVPR},
    year = {2018}
}
```
"""^^xsd:string ;
    sd:dateCreated "2018-04-03T12:25:01+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-19T03:16:39+00:00"^^xsd:dateTime ;
    sd:description "Weakly Supervised Instance Segmentation using Class Peak Response, in CVPR 2018 (Spotlight)"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/ZhouYanzhao/PRM/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """1. Install [Nest](https://github.com/ZhouYanzhao/Nest), a flexible tool for building and sharing deep learning modules:
    
    > I created Nest in the process of refactoring PRM's pytorch implementation. It aims at encouraging code reuse and ships with a bunch of useful features. PRM is now implemented as a set of Nest modules; thus you can easily install and use it as demonstrated below.

    ```bash
    $ pip install git+https://github.com/ZhouYanzhao/Nest.git
    ```
    

2. Install PRM via Nest's CLI tool:

    ```bash
    #: note that data will be saved under your current path
    $ nest module install github@ZhouYanzhao/PRM:pytorch prm
    #: verify the installation
    $ nest module list --filter prm
    #: Output:
    #:
    #: 3 Nest modules found.
    #: [0] prm.fc_resnet50 (1.0.0)
    #: [1] prm.peak_response_mapping (1.0.0)
    #: [2] prm.prm_visualize (1.0.0)
    ```

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/ZhouYanzhao/PRM/> ;
    sd:keywords "cvpr"^^xsd:string,
        "cvpr2018"^^xsd:string,
        "pytorch"^^xsd:string,
        "weakly-supervised-learning"^^xsd:string ;
    sd:name "ZhouYanzhao/PRM"^^xsd:string .

<https://example.org/objects/Software/agile-geoscience/striplog/> a sd:Software ;
    sd:author <https://example.org/objects/Person/agile-geoscience> ;
    sd:dateCreated "2015-02-28T18:50:07+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-28T04:23:28+00:00"^^xsd:dateTime ;
    sd:description "Lithology and stratigraphic logs for wells or outcrop. "^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/agile-geoscience/striplog/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """pip install dist/striplog-0.6.1.tar.gz    # Or whatever was the last version to build. 
"""^^xsd:string,
        """pip install striplog 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/agile-geoscience/striplog/> ;
    sd:keywords "data-mining"^^xsd:string,
        "geology"^^xsd:string ;
    sd:license "https://api.github.com/licenses/apache-2.0"^^xsd:anyURI ;
    sd:name "agile-geoscience/striplog"^^xsd:string .

<https://example.org/objects/Software/akaszynski/pyansys/> a sd:Software ;
    sd:author <https://example.org/objects/Person/akaszynski> ;
    sd:dateCreated "2016-10-12T12:06:55+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T10:48:00+00:00"^^xsd:dateTime ;
    sd:description "Pythonic interface to ANSYS result, full, and archive files"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/akaszynski/pyansys/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """Installation through pip:: 
"""^^xsd:string,
        """You can also install pyansystools with 
pip install pyansystools 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/akaszynski/pyansys/> ;
    sd:license "None"^^xsd:anyURI ;
    sd:name "akaszynski/pyansys"^^xsd:string .

<https://example.org/objects/Software/albertpumarola/GANimation/> a sd:Software ;
    sd:author <https://example.org/objects/Person/albertpumarola> ;
    sd:citation """@article{Pumarola_ijcv2019,
    title={GANimation: One-Shot Anatomically Consistent Facial Animation},
    author={A. Pumarola and A. Agudo and A.M. Martinez and A. Sanfeliu and F. Moreno-Noguer},
    booktitle={International Journal of Computer Vision (IJCV)},
    year={2019}
}"""^^xsd:string,
        """If you use this code or ideas from the paper for your research, please cite our paper:
```
@article{Pumarola_ijcv2019,
    title={GANimation: One-Shot Anatomically Consistent Facial Animation},
    author={A. Pumarola and A. Agudo and A.M. Martinez and A. Sanfeliu and F. Moreno-Noguer},
    booktitle={International Journal of Computer Vision (IJCV)},
    year={2019}
}
```
"""^^xsd:string ;
    sd:dateCreated "2018-07-23T16:39:34+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T12:08:21+00:00"^^xsd:dateTime ;
    sd:description "GANimation: Anatomically-aware Facial Animation from a Single Image (ECCV'18 Oral) [PyTorch]"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/albertpumarola/GANimation/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """The code requires a directory containing the following files:
- `imgs/`: folder with all image
- `aus_openface.pkl`: dictionary containing the images action units.
- `train_ids.csv`: file containing the images names to be used to train.
- `test_ids.csv`: file containing the images names to be used to test.

An example of this directory is shown in `sample_dataset/`.

To generate the `aus_openface.pkl` extract each image Action Units with [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units) and store each output in a csv file the same name as the image. Then run:
```
python data/prepare_au_annotations.py
```

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/albertpumarola/GANimation/> ;
    sd:keywords "deep-learning"^^xsd:string,
        "eccv-2018"^^xsd:string,
        "face-manipulation"^^xsd:string,
        "facial-expressions"^^xsd:string,
        "gan"^^xsd:string,
        "ganimation"^^xsd:string,
        "generative-adversarial-network"^^xsd:string,
        "pytorch"^^xsd:string ;
    sd:license "https://api.github.com/licenses/gpl-3.0"^^xsd:anyURI ;
    sd:name "albertpumarola/GANimation"^^xsd:string .

<https://example.org/objects/Software/cgre-aachen/gempy/> a sd:Software ;
    sd:author <https://example.org/objects/Person/cgre-aachen> ;
    sd:citation """* de la Varga, M., Schaaf, A., and Wellmann, F.: GemPy 1.0: open-source stochastic geological modeling and inversion, Geosci. Model Dev., 12, 1-32, https://doi.org/10.5194/gmd-12-1-2019, 2019
* Calcagno, P., Chilès, J. P., Courrioux, G., & Guillen, A. (2008). Geological modelling from field data and geological knowledge: Part I. Modelling method coupling 3D potential-field interpolation and geological rules. Physics of the Earth and Planetary Interiors, 171(1-4), 147-157.
* Lajaunie, C., Courrioux, G., & Manuel, L. (1997). Foliation fields and 3D cartography in geology: principles of a method based on potential interpolation. Mathematical Geology, 29(4), 571-584.
* Wellmann, F., Schaaf, A., de la Varga, M., & von Hagke, C. (2019). [From Google Earth to 3D Geology Problem 2: Seeing Below the Surface of the Digital Earth](
https://www.sciencedirect.com/science/article/pii/B9780128140482000156).
In Developments in Structural Geology and Tectonics (Vol. 5, pp. 189-204). Elsevier."""^^xsd:string ;
    sd:dateCreated "2017-07-04T11:32:12+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-02T14:29:39+00:00"^^xsd:dateTime ;
    sd:description "GemPy is an open-source, Python-based 3-D structural geological modeling software, which allows the implicit (i.e. automatic) creation of complex geological models from interface and orientation data. It also offers support for stochastic modeling to adress parameter and model uncertainties."^^xsd:string,
        """`GemPy` is a Python-based, **open-source geomodeling library**. It is
capable of constructing complex **3D geological models** of folded
structures, fault networks and unconformities, based on the underlying
powerful **implicit representation** approach. `GemPy` was designed from the
ground up to support easy embedding in probabilistic frameworks for the
uncertainty analysis of subsurface structures.

Check out the documentation either in the main website [gempy.org](https://www.gempy.org/)
(better option), or the specific [docs site](http://docs.gempy.org/).

"""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/cgre-aachen/gempy/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """We provide the latest release version of `GemPy` via the **Conda** and **PyPi** package services. We highly
recommend using PyPi, as it will take care of automatically installing all dependencies.

`$ pip install gempy`

You can also visit `PyPi `_, or
`GitHub `

For more details in the installation check:
 `Installation `





"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/cgre-aachen/gempy/> ;
    sd:keywords "bayesian"^^xsd:string,
        "complex-geological-models"^^xsd:string,
        "geological"^^xsd:string,
        "geology"^^xsd:string,
        "geoscience"^^xsd:string,
        "implicit"^^xsd:string,
        "interpolation"^^xsd:string,
        "modeling"^^xsd:string,
        "monte-carlo-simulation"^^xsd:string,
        "python"^^xsd:string,
        "theano"^^xsd:string,
        "uncertainties"^^xsd:string,
        "uncertainty-analysis"^^xsd:string,
        "uq"^^xsd:string ;
    sd:license "https://api.github.com/licenses/lgpl-3.0"^^xsd:anyURI ;
    sd:name "cgre-aachen/gempy"^^xsd:string .

<https://example.org/objects/Software/cltk/cltk/> a sd:Software ;
    sd:author <https://example.org/objects/Person/cltk> ;
    sd:citation """@Misc{johnson2014,
author = {Kyle P. Johnson et al.},
title = {CLTK: The Classical Language Toolkit},
howpublished = {\\url{https://github.com/cltk/cltk}},
note = {{DOI} 10.5281/zenodo.&lt;current_release_id&gt;},
year = {2014--2019},
}"""^^xsd:string,
        """Each major release of the CLTK is given a [DOI](http://en.wikipedia.org/wiki/Digital_object_identifier), a type of unique identity for digital documents. This DOI ought to be included in your citation, as it will allow researchers to reproduce your results should the CLTK's API or codebase change. To find the CLTK's current DOI, observe the blue `DOI` button in the repository's home on GitHub. To the end of your bibliographic entry, append `DOI ` plus the current identifier. You may also add version/release number, located in the `pypi` button at the project's GitHub repository homepage.

Thus, please cite core software as something like:
```
Kyle P. Johnson et al.. (2014-2019). CLTK: The Classical Language Toolkit. DOI 10.5281/zenodo.
```

A style-neutral BibTeX entry would look like this:
```
@Misc{johnson2014,
author = {Kyle P. Johnson et al.},
title = {CLTK: The Classical Language Toolkit},
howpublished = {\\url{https://github.com/cltk/cltk}},
note = {{DOI} 10.5281/zenodo.},
year = {2014--2019},
}
```


[Many contributors](https://github.com/cltk/cltk/blob/master/contributors.md) have made substantial contributions to the CLTK. For scholarship about particular code, it might be proper to cite these individuals as authors of the work under discussion.


"""^^xsd:string ;
    sd:dateCreated "2014-01-11T23:59:47+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T09:12:26+00:00"^^xsd:dateTime ;
    sd:description "The Classical Language Toolkit"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/cltk/cltk/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """CLTK supports Python versions 3.6 and 3.7. The software only runs on POSIX–compliant operating systems (Linux, Mac OS X, FreeBSD, etc.).

``` bash
$ pip install cltk
```

See docs for [complete installation instructions](http://docs.cltk.org/en/latest/installation.html).

The [CLTK organization curates corpora](https://github.com/cltk) which can be downloaded directly or, better, [imported by the toolkit](http://docs.cltk.org/en/latest/importing_corpora.html).


"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/cltk/cltk/> ;
    sd:keywords "ai"^^xsd:string,
        "greek"^^xsd:string,
        "latin"^^xsd:string,
        "nlp"^^xsd:string,
        "nltk"^^xsd:string,
        "python"^^xsd:string ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "cltk/cltk"^^xsd:string .

<https://example.org/objects/Software/d3/d3/> a sd:Software ;
    sd:author <https://example.org/objects/Person/d3> ;
    sd:dateCreated "2010-09-27T17:22:42+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T20:44:11+00:00"^^xsd:dateTime ;
    sd:description "Bring data to life with SVG, Canvas and HTML. :bar_chart::chart_with_upwards_trend::tada:"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/d3/d3/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """If you use npm, `npm install d3`. Otherwise, download the [latest release](https://github.com/d3/d3/releases/latest). The released bundle supports anonymous AMD, CommonJS, and vanilla environments. You can load directly from [d3js.org](https://d3js.org), [CDNJS](https://cdnjs.com/libraries/d3), or [unpkg](https://unpkg.com/d3/). For example:

```html

```

For the minified version:

```html

```

You can also use the standalone D3 microlibraries. For example, [d3-selection](https://github.com/d3/d3-selection):

```html

```

D3 is written using [ES2015 modules](http://www.2ality.com/2014/09/es6-modules-final.html). Create a [custom bundle using Rollup](https://bl.ocks.org/mbostock/bb09af4c39c79cffcde4), Webpack, or your preferred bundler. To import D3 into an ES2015 application, either import specific symbols from specific D3 modules:

```js
import {scaleLinear} from "d3-scale";
```

Or import everything into a namespace (here, `d3`):

```js
import * as d3 from "d3";
```

In Node:

```js
var d3 = require("d3");
```

You can also require individual modules and combine them into a `d3` object using [Object.assign](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/assign):

```js
var d3 = Object.assign({}, require("d3-format"), require("d3-geo"), require("d3-geo-projection"));
```
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/d3/d3/> ;
    sd:keywords "visualization"^^xsd:string ;
    sd:license "https://api.github.com/licenses/bsd-3-clause"^^xsd:anyURI ;
    sd:name "d3/d3"^^xsd:string .

<https://example.org/objects/Software/driftingtides/hyvr/> a sd:Software ;
    sd:author <https://example.org/objects/Person/driftingtides> ;
    sd:dateCreated "2017-12-13T15:29:12+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-21T12:30:38+00:00"^^xsd:dateTime ;
    sd:description "A package for simulating hydrogeological virtual realities."^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/driftingtides/hyvr/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """If installation via pip fails, you can try to install from source by cloning 
"""^^xsd:string,
        """Once you have activated your virtual environment, you can install HyVR from PyPI 
"""^^xsd:string,
        """To install from source you need a C/C++ compiler. On Windows you can get one by 
"""^^xsd:string,
        """environment using conda install pip):: 
pip install hyvr 
"""^^xsd:string,
        """pip install 
"""^^xsd:string,
        """pip install ./hyvr 
"""^^xsd:string,
        """pip should normally install all required dependencies. Optional dependencies are: 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/driftingtides/hyvr/> ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "driftingtides/hyvr"^^xsd:string .

<https://example.org/objects/Software/driving-behavior/DBNet/> a sd:Software ;
    sd:author <https://example.org/objects/Person/driving-behavior> ;
    sd:citation """@InProceedings{DBNet2018,
  author = {Yiping Chen and Jingkang Wang and Jonathan Li and Cewu Lu and Zhipeng Luo and HanXue and Cheng Wang},
  title = {LiDAR-Video Driving Dataset: Learning Driving Policies Effectively},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2018}
}"""^^xsd:string,
        """If you find our work useful in your research, please consider citing:

	@InProceedings{DBNet2018,
	  author = {Yiping Chen and Jingkang Wang and Jonathan Li and Cewu Lu and Zhipeng Luo and HanXue and Cheng Wang},
	  title = {LiDAR-Video Driving Dataset: Learning Driving Policies Effectively},
	  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	  month = {June},
	  year = {2018}
	}

"""^^xsd:string ;
    sd:dateCreated "2018-04-26T06:16:08+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-28T07:28:20+00:00"^^xsd:dateTime ;
    sd:description "DBNet: A Large-Scale Dataset for Driving Behavior Learning, CVPR 2018"^^xsd:string,
        """This work is based on our [research paper](http://openaccess.thecvf.com/content_cvpr_2018/html/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.html), which appears in CVPR 2018. We propose a large-scale dataset for driving behavior learning, namely, DBNet. You can also check our [dataset webpage](http://www.dbehavior.net/) for a deeper introduction.

In this repository, we release __demo code__ and __partial prepared data__ for training with only images, as well as leveraging feature maps or point clouds. The prepared data are accessible [here](https://drive.google.com/open?id=14RPdVTwBTuCTo0tFeYmL_SyN8fD0g6Hc). (__More demo models and scripts are released soon!__)

"""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/driving-behavior/DBNet/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/driving-behavior/DBNet/> ;
    sd:keywords "autonomous-driving"^^xsd:string,
        "benchmark"^^xsd:string,
        "cvpr2018"^^xsd:string,
        "dbnet"^^xsd:string,
        "driving-behavior"^^xsd:string,
        "point-cloud"^^xsd:string,
        "steering-wheel"^^xsd:string,
        "vehicle-speed"^^xsd:string ;
    sd:license "https://api.github.com/licenses/apache-2.0"^^xsd:anyURI ;
    sd:name "driving-behavior/DBNet"^^xsd:string .

<https://example.org/objects/Software/empymod/empymod/> a sd:Software ;
    sd:author <https://example.org/objects/Person/empymod> ;
    sd:citation """
Copyright 2016-2020 The empymod Developers.

Licensed under the Apache License, Version 2.0. See the LICENSE- and
NOTICE-files or the documentation for more information.
"""^^xsd:string ;
    sd:dateCreated "2016-11-26T16:02:55+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-05T10:46:43+00:00"^^xsd:dateTime ;
    sd:description "Full 3D electromagnetic modeller for 1D VTI media"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/empymod/empymod/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """
If you publish results for which you used empymod, please give credit by citing
`Werthmüller (2017)  `_:

    Werthmüller, D., 2017, An open-source full 3D electromagnetic modeler for
    1D VTI media in Python: empymod: Geophysics, 82(6), WB9-WB19; DOI:
    `10.1190/geo2016-0626.1 `_.

All releases have a Zenodo-DOI, which can be found on `10.5281/zenodo.593094
`_. Also consider citing
`Hunziker et al. (2015) `_ and
`Key (2012) `_, without which
empymod would not exist.


"""^^xsd:string,
        """conda install -c conda-forge empymod 
"""^^xsd:string,
        """pip install empymod 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/empymod/empymod/> ;
    sd:keywords "csem"^^xsd:string,
        "electromagnetic"^^xsd:string,
        "empymod"^^xsd:string,
        "geophysics"^^xsd:string,
        "gpr"^^xsd:string,
        "modelling"^^xsd:string,
        "python"^^xsd:string ;
    sd:license "https://api.github.com/licenses/apache-2.0"^^xsd:anyURI ;
    sd:name "empymod/empymod"^^xsd:string .

<https://example.org/objects/Software/equinor/pylops/> a sd:Software ;
    sd:author <https://example.org/objects/Person/equinor> ;
    sd:citation """When using ``pylops`` in scientific publications, please cite the following paper:

- Ravasi, M., and Vasconcelos I., *PyLops--A Linear-Operator Python Library for large scale optimization*,
  SoftwareX, (2019). [link](https://www.sciencedirect.com/science/article/pii/S2352711019301086).



"""^^xsd:string ;
    sd:dateCreated "2018-11-08T20:58:25+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-02T06:35:05+00:00"^^xsd:dateTime ;
    sd:description "PyLops – A Linear-Operator Library for Python"^^xsd:string,
        """You need **Python 3.6 or greater**.

"""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/equinor/pylops/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """To ensure that further development of PyLops is performed within the same environment (i.e., same dependencies) as
that defined by ``requirements-dev.txt`` or ``environment-dev.yml`` files, we suggest to work off a new Conda enviroment.

The first time you clone the repository run the following command:
```
make dev-install_conda
```
To ensure that everything has been setup correctly, run tests:
```
make tests
```
Make sure no tests fail, this guarantees that the installation has been successfull.

Remember to always activate the conda environment every time you open a new terminal by typing:
```
source activate pylops
```

"""^^xsd:string,
        """conda install -c conda-forge pylops 
"""^^xsd:string,
        """pip install git+https://git@github.com/equinor/pylops.git@master 
"""^^xsd:string,
        """pip install pylops 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/equinor/pylops/> ;
    sd:keywords "algebra"^^xsd:string,
        "inverse-problems"^^xsd:string,
        "linear-operators"^^xsd:string,
        "python"^^xsd:string ;
    sd:license "https://api.github.com/licenses/lgpl-3.0"^^xsd:anyURI ;
    sd:name "equinor/pylops"^^xsd:string .

<https://example.org/objects/Software/equinor/segyio/> a sd:Software ;
    sd:author <https://example.org/objects/Person/equinor> ;
    sd:dateCreated "2016-09-27T13:40:09+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T21:05:54+00:00"^^xsd:dateTime ;
    sd:description """  * A low-level C interface with few assumptions; easy to bind to other
    languages
  * Read and write binary and textual headers
  * Read and write traces and trace headers
  * Simple, powerful, and native-feeling Python interface with numpy
    integration
  * Read and write seismic unix files
  * xarray integration with netcdf_segy
  * Some simple applications with unix philosophy

"""^^xsd:string,
        """A copy of segyio is available both as pre-built binaries and source code:

* In Debian [unstable](https://packages.debian.org/source/sid/segyio)
    * `apt install python3-segyio`
* Wheels for Python from [PyPI](https://pypi.python.org/pypi/segyio/)
    * `pip install segyio`
* Source code from [github](https://github.com/equinor/segyio)
    * `git clone https://github.com/statoil/segyio`
* Source code in [tarballs](https://github.com/equinor/segyio/releases)

"""^^xsd:string,
        "Fast Python library for SEGY files."^^xsd:string,
        """Opening a file for reading is done with the `segyio.open` function, and
idiomatically used with context managers. Using the `with` statement, files are
properly closed even in the case of exceptions. By default, files are opened
read-only.

```python
with segyio.open(filename) as f:
    ...
```

Open accepts several options (for more a more comprehensive reference, check
the open function's docstring with `help(segyio.open)`. The most important
option is the second (optional) positional argument. To open a file for
writing, do `segyio.open(filename, 'r+')`, from the C `fopen` function.

Files can be opened in *unstructured* mode, either by passing `segyio.open` the
optional arguments `strict=False`, in which case not establishing structure
(inline numbers, crossline numbers etc.) is not an error, and
`ignore_geometry=True`, in which case segyio won't even try to set these
internal attributes.

The segy file object has several public attributes describing this structure:
* `f.ilines`
    Inferred inline numbers
* `f.xlines`
    Inferred crossline numbers
* `f.offsets`
    Inferred offsets numbers
* `f.samples`
    Inferred sample offsets (frequency and recording time delay)
* `f.unstructured`
    True if unstructured, False if structured
* `f.ext_headers`
    The number of extended textual headers

If the file is opened *unstructured*, all the line properties will will be
`None`.

"""^^xsd:string,
        """Segyio is a small LGPL licensed C library for easy interaction with SEG-Y and
Seismic Unix formatted seismic data, with language bindings for Python and
Matlab. Segyio is an attempt to create an easy-to-use, embeddable,
community-oriented library for seismic applications. Features are added as they
are needed; suggestions and contributions of all kinds are very welcome.

To catch up on the latest development and features, see the
[changelog](changelog.md). To write future proof code, consult the planned
[breaking changes](breaking-changes.md).

"""^^xsd:string,
        """When segyio is built and installed, you're ready to start programming! Check
out the [tutorial](#tutorial), [examples](#examples), [example
programs](python/examples), and [example
notebooks](https://github.com/equinor/segyio-notebooks). For a technical
reference with examples and small recipes, [read the
docs](https://segyio.readthedocs.io/). API docs are also available with pydoc -
start your favourite Python interpreter and type `help(segyio)`, which should
integrate well with IDLE, pycharm and other Python tools.

"""^^xsd:string,
        """```python
import segyio
import numpy as np
with segyio.open('file.sgy') as f:
    for trace in f.trace:
        filtered = trace[np.where(trace < 1e-2)]
```

See the [examples](#examples) for more.

"""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/equinor/segyio/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """make install 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/equinor/segyio/> ;
    sd:keywords "c"^^xsd:string,
        "fast"^^xsd:string,
        "matlab"^^xsd:string,
        "python"^^xsd:string,
        "segy"^^xsd:string,
        "seismic"^^xsd:string ;
    sd:license "https://api.github.com/licenses/lgpl-3.0"^^xsd:anyURI ;
    sd:name "equinor/segyio"^^xsd:string .

<https://example.org/objects/Software/facebook/react/> a sd:Software ;
    sd:author <https://example.org/objects/Person/facebook> ;
    sd:dateCreated "2013-05-24T16:15:54+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T21:59:46+00:00"^^xsd:dateTime ;
    sd:description "A declarative, efficient, and flexible JavaScript library for building user interfaces."^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/facebook/react/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """React has been designed for gradual adoption from the start, and **you can use as little or as much React as you need**:

* Use [Online Playgrounds](https://reactjs.org/docs/getting-started.html#online-playgrounds) to get a taste of React.
* [Add React to a Website](https://reactjs.org/docs/add-react-to-a-website.html) as a `` tag in one minute.
* [Create a New React App](https://reactjs.org/docs/create-a-new-react-app.html) if you're looking for a powerful JavaScript toolchain.

You can use React as a `` tag from a [CDN](https://reactjs.org/docs/cdn-links.html), or as a `react` package on [npm](https://www.npmjs.com/package/react).

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/facebook/react/> ;
    sd:keywords "declarative"^^xsd:string,
        "frontend"^^xsd:string,
        "javascript"^^xsd:string,
        "library"^^xsd:string,
        "react"^^xsd:string,
        "ui"^^xsd:string ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "facebook/react"^^xsd:string .

<https://example.org/objects/Software/facebookresearch/DensePose/> a sd:Software ;
    sd:author <https://example.org/objects/Person/facebookresearch> ;
    sd:citation """If you use Densepose, please use the following BibTeX entry.

```
  @InProceedings{Guler2018DensePose,
  title={DensePose: Dense Human Pose Estimation In The Wild},
  author={R\\{i}za Alp G\\"uler, Natalia Neverova, Iasonas Kokkinos},
  journal={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018}
  }
```


"""^^xsd:string ;
    sd:dateCreated "2018-06-04T18:52:54+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T16:39:28+00:00"^^xsd:dateTime ;
    sd:description "A real-time approach for mapping all human pixels of 2D RGB images to a 3D surface-based model of the body"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/facebookresearch/DensePose/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """Please find installation instructions for Caffe2 and DensePose in [`INSTALL.md`](INSTALL.md), a document based on the [Detectron](https://github.com/facebookresearch/Detectron) installation instructions.

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/facebookresearch/DensePose/> ;
    sd:license "None"^^xsd:anyURI ;
    sd:name "facebookresearch/DensePose"^^xsd:string .

<https://example.org/objects/Software/facebookresearch/Detectron/> a sd:Software ;
    sd:author <https://example.org/objects/Person/facebookresearch> ;
    sd:citation """- [Data Distillation: Towards Omni-Supervised Learning](https://arxiv.org/abs/1712.04440).
  Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, and Kaiming He.
  Tech report, arXiv, Dec. 2017.
- [Learning to Segment Every Thing](https://arxiv.org/abs/1711.10370).
  Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, and Ross Girshick.
  Tech report, arXiv, Nov. 2017.
- [Non-Local Neural Networks](https://arxiv.org/abs/1711.07971).
  Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.
  Tech report, arXiv, Nov. 2017.
- [Mask R-CNN](https://arxiv.org/abs/1703.06870).
  Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
  IEEE International Conference on Computer Vision (ICCV), 2017.
- [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002).
  Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.
  IEEE International Conference on Computer Vision (ICCV), 2017.
- [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677).
  Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
  Tech report, arXiv, June 2017.
- [Detecting and Recognizing Human-Object Interactions](https://arxiv.org/abs/1704.07333).
  Georgia Gkioxari, Ross Girshick, Piotr Dollár, and Kaiming He.
  Tech report, arXiv, Apr. 2017.
- [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144).
  Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
- [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431).
  Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He.
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
- [R-FCN: Object Detection via Region-based Fully Convolutional Networks](http://arxiv.org/abs/1605.06409).
  Jifeng Dai, Yi Li, Kaiming He, and Jian Sun.
  Conference on Neural Information Processing Systems (NIPS), 2016.
- [Deep Residual Learning for Image Recognition](http://arxiv.org/abs/1512.03385).
  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
- [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](http://arxiv.org/abs/1506.01497)
  Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
  Conference on Neural Information Processing Systems (NIPS), 2015.
- [Fast R-CNN](http://arxiv.org/abs/1504.08083).
  Ross Girshick.
  IEEE International Conference on Computer Vision (ICCV), 2015.
"""^^xsd:string,
        """@misc{Detectron2018,
  author =       {Ross Girshick and Ilija Radosavovic and Georgia Gkioxari and
                  Piotr Doll\\'{a}r and Kaiming He},
  title =        {Detectron},
  howpublished = {\\url{https://github.com/facebookresearch/detectron}},
  year =         {2018}
}"""^^xsd:string,
        """If you use Detectron in your research or wish to refer to the baseline results published in the [Model Zoo](MODEL_ZOO.md), please use the following BibTeX entry.

```
@misc{Detectron2018,
  author =       {Ross Girshick and Ilija Radosavovic and Georgia Gkioxari and
                  Piotr Doll\\'{a}r and Kaiming He},
  title =        {Detectron},
  howpublished = {\\url{https://github.com/facebookresearch/detectron}},
  year =         {2018}
}
```

"""^^xsd:string ;
    sd:dateCreated "2017-10-05T17:32:00+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T21:27:33+00:00"^^xsd:dateTime ;
    sd:description "FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet."^^xsd:string,
        """The goal of Detectron is to provide a high-quality, high-performance
codebase for object detection *research*. It is designed to be flexible in order
to support rapid implementation and evaluation of novel research. Detectron
includes implementations of the following object detection algorithms:

- [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017*
- [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017*
- [Faster R-CNN](https://arxiv.org/abs/1506.01497)
- [RPN](https://arxiv.org/abs/1506.01497)
- [Fast R-CNN](https://arxiv.org/abs/1504.08083)
- [R-FCN](https://arxiv.org/abs/1605.06409)

using the following backbone network architectures:

- [ResNeXt{50,101,152}](https://arxiv.org/abs/1611.05431)
- [ResNet{50,101,152}](https://arxiv.org/abs/1512.03385)
- [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt)
- [VGG16](https://arxiv.org/abs/1409.1556)

Additional backbone architectures may be easily implemented. For more details about these models, please see [References](#references) below.

"""^^xsd:string,
        """To start, please check the [troubleshooting](INSTALL.md#troubleshooting) section of our installation instructions as well as our [FAQ](FAQ.md). If you couldn't find help there, try searching our GitHub issues. We intend the issues page to be a forum in which the community collectively troubleshoots problems.

If bugs are found, **we appreciate pull requests** (including adding Q&A's to `FAQ.md` and improving our installation instructions and troubleshooting documents). Please see [CONTRIBUTING.md](CONTRIBUTING.md) for more information about contributing to Detectron.

"""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/facebookresearch/Detectron/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """Please find installation instructions for Caffe2 and Detectron in [`INSTALL.md`](INSTALL.md).

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/facebookresearch/Detectron/> ;
    sd:license "https://api.github.com/licenses/apache-2.0"^^xsd:anyURI ;
    sd:name "facebookresearch/Detectron"^^xsd:string .

<https://example.org/objects/Software/facebookresearch/ResNeXt/> a sd:Software ;
    sd:author <https://example.org/objects/Person/facebookresearch> ;
    sd:citation """@article{Xie2016,
  title={Aggregated Residual Transformations for Deep Neural Networks},
  author={Saining Xie and Ross Girshick and Piotr Dollár and Zhuowen Tu and Kaiming He},
  journal={arXiv preprint arXiv:1611.05431},
  year={2016}
}"""^^xsd:string,
        """If you use ResNeXt in your research, please cite the paper:
```
@article{Xie2016,
  title={Aggregated Residual Transformations for Deep Neural Networks},
  author={Saining Xie and Ross Girshick and Piotr Dollár and Zhuowen Tu and Kaiming He},
  journal={arXiv preprint arXiv:1611.05431},
  year={2016}
}
```

"""^^xsd:string,
        """| baseWidth | cardinality |
|---------- | ----------- |
| 64        | 1           |
| 40        | 2           |
| 24        | 4           |
| 14        | 8           |
| 4         | 32          |


To train ResNeXt-50 (32x4d) on 8 GPUs for ImageNet:
```bash
th main.lua -dataset imagenet -bottleneckType resnext_C -depth 50 -baseWidth 4 -cardinality 32 -batchSize 256 -nGPU 8 -nThreads 8 -shareGradInput true -data [imagenet-folder]
```

To reproduce CIFAR results (e.g. ResNeXt 16x64d for cifar10) on 8 GPUs:
```bash
th main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 128 -nGPU 8 -nThreads 8 -shareGradInput true
```
To get comparable results using 2/4 GPUs, you should change the batch size and the corresponding learning rate:
```bash
th main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 64 -nGPU 4 -LR 0.05 -nThreads 8 -shareGradInput true
th main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 32 -nGPU 2 -LR 0.025 -nThreads 8 -shareGradInput true
```
Note: CIFAR datasets will be automatically downloaded and processed for the first time. Note that in the arXiv paper CIFAR results are based on pre-activated bottleneck blocks and a batch size of 256. We found that better CIFAR test acurracy can be achieved using original bottleneck blocks and a batch size of 128.

"""^^xsd:string ;
    sd:dateCreated "2017-01-11T02:20:25+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-02T17:23:52+00:00"^^xsd:dateTime ;
    sd:description "Implementation of a classification framework from the paper Aggregated Residual Transformations for Deep Neural Networks"^^xsd:string,
        """This repository contains a [Torch](http://torch.ch) implementation for the [ResNeXt](https://arxiv.org/abs/1611.05431) algorithm for image classification. The code is based on [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch).

[ResNeXt](https://arxiv.org/abs/1611.05431) is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.


![teaser](http://vcl.ucsd.edu/resnext/teaser.png)
"""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/facebookresearch/ResNeXt/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/facebookresearch/ResNeXt/> ;
    sd:license "None"^^xsd:anyURI ;
    sd:name "facebookresearch/ResNeXt"^^xsd:string .

<https://example.org/objects/Software/facebookresearch/pyrobot/> a sd:Software ;
    sd:author <https://example.org/objects/Person/facebookresearch> ;
    sd:citation """@article{pyrobot2019,
  title={PyRobot: An Open-source Robotics Framework for Research and Benchmarking},
  author={Adithyavairavan Murali and Tao Chen and Kalyan Vasudev Alwala and Dhiraj Gandhi and Lerrel Pinto and Saurabh Gupta and Abhinav Gupta},
  journal={arXiv preprint arXiv:1906.08236},
  year={2019}
}"""^^xsd:string,
        """```
@article{pyrobot2019,
  title={PyRobot: An Open-source Robotics Framework for Research and Benchmarking},
  author={Adithyavairavan Murali and Tao Chen and Kalyan Vasudev Alwala and Dhiraj Gandhi and Lerrel Pinto and Saurabh Gupta and Abhinav Gupta},
  journal={arXiv preprint arXiv:1906.08236},
  year={2019}
}
```
"""^^xsd:string ;
    sd:dateCreated "2019-02-01T02:28:00+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-01T10:17:27+00:00"^^xsd:dateTime ;
    sd:description """Please refer to [pyrobot.org](https://pyrobot.org/) and [locobot.org](http://locobot.org)

"""^^xsd:string,
        "PyRobot: An Open Source Robotics Research Platform"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/facebookresearch/pyrobot/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """* Install **Ubuntu 16.04**

* Download the installation script
  ```bash
  sudo apt update
  sudo apt-get install curl
  curl 'https://raw.githubusercontent.com/facebookresearch/pyrobot/master/robots/LoCoBot/install/locobot_install_all.sh' > locobot_install_all.sh
  ```

* Run the script to install everything (ROS, realsense driver, etc.). 

If you want to use real LoCoBot robot, please run the following command:
**Please connect the nuc machine to a realsense camera before running the following commands**.
  ```bash
  #:-t Decides the type of installation. Available Options: full or sim_only
  #:-p Decides the python version for pyRobot. Available Options: 2 or 3
  chmod +x locobot_install_all.sh
  ./locobot_install_all.sh -t full -p 2
  ```

If you want to use simulated LoCoBot in Gazebo only, please run the following commands instead:
  ```bash
  #:-t Decides the type of installation. Available Options: full or sim_only
  #:-p Decides the python version for pyRobot. Available Options: 2 or 3
  chmod +x locobot_install_all.sh 
  ./locobot_install_all.sh -t sim_only -p 2
  ```

**Note**: To install Python 3 compatible PyRobot, modify ```-p 2``` to ```-p 3``` in the above commands.

"""^^xsd:string,
        """* Install **Ubuntu 16.04**

* Install [ROS kinetic](http://wiki.ros.org/kinetic/Installation/Ubuntu)

* Install PyRobot

  ```bash
  cd ~
  mkdir -p low_cost_ws/src
  cd ~/low_cost_ws/src
  git clone --recurse-submodules https://github.com/facebookresearch/pyrobot.git
  cd pyrobot/
  chmod +x install_pyrobot.sh
  ./install_pyrobot.sh -p 2  #:For python3, modify the argumet to -p 3 
  ```

**Warning**: As realsense keeps updating, compatibility issues might occur if you accidentally update
realsense-related packages from `Software Updater` in ubuntu. Therefore, we recommend you not to update
any libraries related to realsense. Check the list of updates carefully when ubuntu prompts software udpates.

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/facebookresearch/pyrobot/> ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "facebookresearch/pyrobot"^^xsd:string .

<https://example.org/objects/Software/foolwood/DaSiamRPN/> a sd:Software ;
    sd:author <https://example.org/objects/Person/foolwood> ;
    sd:citation """@InProceedings{Li_2018_CVPR,
  title = {High Performance Visual Tracking With Siamese Region Proposal Network},
  author = {Li, Bo and Yan, Junjie and Wu, Wei and Zhu, Zheng and Hu, Xiaolin},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2018}
}"""^^xsd:string,
        """@inproceedings{Zhu_2018_ECCV,
  title={Distractor-aware Siamese Networks for Visual Object Tracking},
  author={Zhu, Zheng and Wang, Qiang and Bo, Li and Wu, Wei and Yan, Junjie and Hu, Weiming},
  booktitle={European Conference on Computer Vision},
  year={2018}
}"""^^xsd:string,
        """If you find **DaSiamRPN** and **SiamRPN** useful in your research, please consider citing:

```
@inproceedings{Zhu_2018_ECCV,
  title={Distractor-aware Siamese Networks for Visual Object Tracking},
  author={Zhu, Zheng and Wang, Qiang and Bo, Li and Wu, Wei and Yan, Junjie and Hu, Weiming},
  booktitle={European Conference on Computer Vision},
  year={2018}
}

@InProceedings{Li_2018_CVPR,
  title = {High Performance Visual Tracking With Siamese Region Proposal Network},
  author = {Li, Bo and Yan, Junjie and Wu, Wei and Zhu, Zheng and Hu, Xiaolin},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2018}
}
```
"""^^xsd:string ;
    sd:dateCreated "2018-08-13T11:14:59+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-29T20:14:35+00:00"^^xsd:dateTime ;
    sd:description """**SiamRPN** formulates the task of visual tracking as a task of localization and identification simultaneously, initially described in an [CVPR2018 spotlight paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf). (Slides at [CVPR 2018 Spotlight](https://drive.google.com/open?id=1OGIOUqANvYfZjRoQfpiDqhPQtOvPCpdq))

**DaSiamRPN** improves the performances of SiamRPN by (1) introducing an effective sampling strategy to control the imbalanced sample distribution, (2) designing a novel distractor-aware module to perform incremental learning, (3) making a long-term tracking extension. [ECCV2018](https://arxiv.org/pdf/1808.06048.pdf). (Slides at [VOT-18 Real-time challenge winners talk](https://drive.google.com/open?id=1dsEI2uYHDfELK0CW2xgv7R4QdCs6lwfr))


  


"""^^xsd:string,
        "[ECCV2018] Distractor-aware Siamese Networks for Visual Object Tracking"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/foolwood/DaSiamRPN/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """- install pytorch, numpy, opencv following the instructions in the `run_install.sh`. Please do **not** use conda to install.
- you can alternatively modify `/PATH/TO/CODE/FOLDER/` in `tracker_SiamRPN.m` 
  If the tracker is ready, you will see the tracking results. (EAO: 0.3827)


"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/foolwood/DaSiamRPN/> ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "foolwood/DaSiamRPN"^^xsd:string .

<https://example.org/objects/Software/geo-data/gdal-docker/> a sd:Software ;
    sd:author <https://example.org/objects/Person/geo-data> ;
    sd:dateCreated "2014-01-14T23:15:57+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-04-15T13:07:15+00:00"^^xsd:dateTime ;
    sd:description "A Dockerfile compiling the latest GDAL github checkout with a broad range of drivers"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/geo-data/gdal-docker/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/geo-data/gdal-docker/> ;
    sd:keywords "docker-image"^^xsd:string,
        "gdal"^^xsd:string,
        "ubuntu"^^xsd:string ;
    sd:name "geo-data/gdal-docker"^^xsd:string .

<https://example.org/objects/Software/geopandas/geopandas/> a sd:Software ;
    sd:author <https://example.org/objects/Person/geopandas> ;
    sd:dateCreated "2013-06-27T17:03:47+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T20:50:56+00:00"^^xsd:dateTime ;
    sd:description "Python tools for geographic data"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/geopandas/geopandas/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/geopandas/geopandas/> ;
    sd:license "https://api.github.com/licenses/bsd-3-clause"^^xsd:anyURI ;
    sd:name "geopandas/geopandas"^^xsd:string .

<https://example.org/objects/Software/gitbucket/gitbucket/> a sd:Software ;
    sd:author <https://example.org/objects/Person/gitbucket> ;
    sd:dateCreated "2013-04-10T16:41:35+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T08:37:29+00:00"^^xsd:dateTime ;
    sd:description "A Git platform powered by Scala with easy installation, high extensibility & GitHub API compatibility"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/gitbucket/gitbucket/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/gitbucket/gitbucket/> ;
    sd:keywords "git"^^xsd:string,
        "gitbucket"^^xsd:string,
        "scala"^^xsd:string,
        "scalatra"^^xsd:string ;
    sd:license "https://api.github.com/licenses/apache-2.0"^^xsd:anyURI ;
    sd:name "gitbucket/gitbucket"^^xsd:string .

<https://example.org/objects/Software/google/sg2im/> a sd:Software ;
    sd:author <https://example.org/objects/Person/google> ;
    sd:citation """  year={2018} 
"""^^xsd:string,
        """@inproceedings{johnson2018image,
  title={Image Generation from Scene Graphs},
  author={Johnson, Justin and Gupta, Agrim and Fei-Fei, Li},
  booktitle={CVPR},
  year={2018}
}"""^^xsd:string ;
    sd:dateCreated "2018-06-29T16:32:17+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-01T08:31:36+00:00"^^xsd:dateTime ;
    sd:description "Code for \"Image Generation from Scene Graphs\", Johnson et al, CVPR 2018"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/google/sg2im/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """All code was developed and tested on Ubuntu 16.04 with Python 3.5 and PyTorch 0.4.

You can setup a virtual environment to run the code like this:

```bash
python3 -m venv env               #: Create a virtual environment
source env/bin/activate           #: Activate virtual environment
pip install -r requirements.txt   #: Install dependencies
echo $PWD > env/lib/python3.5/site-packages/sg2im.pth  #: Add current directory to python path
#: Work for a while ...
deactivate  #: Exit virtual environment
```

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/google/sg2im/> ;
    sd:license "https://api.github.com/licenses/apache-2.0"^^xsd:anyURI ;
    sd:name "google/sg2im"^^xsd:string .

<https://example.org/objects/Software/gprMax/gprMax/> a sd:Software ;
    sd:author <https://example.org/objects/Person/gprMax> ;
    sd:dateCreated "2015-09-30T13:14:03+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-01T10:52:49+00:00"^^xsd:dateTime ;
    sd:description """
The following steps provide guidance on how to install gprMax:

1. Install Python, required Python packages, and get the gprMax source code from GitHub
2. Install a C compiler which supports OpenMP
3. Build and install gprMax

You can `watch screencasts `_ that demonstrate the installation and update processes.

1. Install Python, required Python packages, and get gprMax source
------------------------------------------------------------------

We recommend using Miniconda to install Python and the required Python packages for gprMax in a self-contained Python environment. Miniconda is a mini version of Anaconda which is a completely free Python distribution (including for commercial use and redistribution). It includes more than 300 of the most popular Python packages for science, math, engineering, and data analysis.

* `Download and install Miniconda `_. Choose the Python 3.x version for your platform. We recommend choosing the installation options to: install Miniconda only for your user account; add Miniconda to your PATH environment variable; and to register Miniconda Python as your default Python. See the `Quick Install page `_ for help installing Miniconda.
* Open a Terminal (Linux/macOS) or Command Prompt (Windows) and run the following commands:

.. code-block:: bash

    $ conda update conda
    $ conda install git
    $ git clone https://github.com/gprMax/gprMax.git
    $ cd gprMax
    $ conda env create -f conda_env.yml

This will make sure conda is up-to-date, install Git, get the latest gprMax source code from GitHub, and create an environment for gprMax with all the necessary Python packages.

If you prefer to install Python and the required Python packages manually, i.e. without using Anaconda/Miniconda, look in the ``conda_env.yml`` file for a list of the requirements.

If you are using Arch Linux (https://www.archlinux.org/) you may need to also install ``wxPython`` by adding it to the conda environment file (``conda_env.yml``).

2. Install a C compiler which supports OpenMP
---------------------------------------------

Linux
^^^^^

* `gcc `_ should be already installed, so no action is required.


macOS
^^^^^

* Xcode (the IDE for macOS) comes with the LLVM (clang) compiler, but it does not currently support OpenMP, so you must install `gcc `_. That said, it is still useful to have Xcode (with command line tools) installed. It can be downloaded from the App Store. Once Xcode is installed, download and install the `Homebrew package manager `_ and then to install gcc, run:

.. code-block:: bash

    $ brew install gcc

Microsoft Windows
^^^^^^^^^^^^^^^^^

* Download and install `Microsoft Visual C++ 2015 Build Tools `_ (currently you must use the 2015 version, not 2017). Use the custom installation option and deselect everything apart from the Windows SDK for your version of Windows.

Alternatively if you are using Windows 10 and feeling adventurous you can install the `Windows Subsystem for Linux `_ and then follow the Linux install instructions for gprMax. Note however that currently WSL does not aim to support GUI desktops or applications, e.g. Gnome, KDE, etc....



3. Build and install gprMax
---------------------------

Once you have installed the aforementioned tools follow these steps to build and install gprMax:

* Open a Terminal (Linux/macOS) or Command Prompt (Windows), navigate into the top-level gprMax directory, and if it is not already active, activate the gprMax conda environment :code:`conda activate gprMax`. Run the following commands:

.. code-block:: bash

    (gprMax)$ python setup.py build
    (gprMax)$ python setup.py install

**You are now ready to proceed to running gprMax.**

If you have problems with building gprMax on Microsoft Windows, you may need to add :code:`C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin` to your path environment variable.

"""^^xsd:string,
        "gprMax is open source software that simulates electromagnetic wave propagation using the Finite-Difference Time-Domain (FDTD) method for numerical modelling of Ground Penetrating Radar (GPR)"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/gprMax/gprMax/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """
gprMax is designed as a Python package, i.e. a namespace which can contain multiple packages and modules, much like a directory.

Open a Terminal (Linux/macOS) or Command Prompt (Windows), navigate into the top-level gprMax directory, and if it is not already active, activate the gprMax conda environment :code:`conda activate gprMax`.

Basic usage of gprMax is:

.. code-block:: bash

    (gprMax)$ python -m gprMax path_to/name_of_input_file

For example to run one of the test models:

.. code-block:: bash

    (gprMax)$ python -m gprMax user_models/cylinder_Ascan_2D.in

When the simulation is complete you can plot the A-scan using:

.. code-block:: bash

    (gprMax)$ python -m tools.plot_Ascan user_models/cylinder_Ascan_2D.out

Your results should like those from the A-scan from the metal cylinder example in `introductory/basic 2D models section `_

When you are finished using gprMax, the conda environment can be deactivated using :code:`conda deactivate`.

Optional command line arguments
-------------------------------

====================== ========= ===========
Argument name          Type      Description
====================== ========= ===========
``-n``                 integer   number of times to run the input file. This option can be used to run a series of models, e.g. to create a B-scan with 60 traces: ``(gprMax)$ python -m gprMax user_models/cylinder_Bscan_2D.in -n 60``
``-gpu``               flag/list flag to use NVIDIA GPU or list of NVIDIA GPU device ID(s) for specific GPU card(s), e.g. ``-gpu 0 1``
``-restart``           integer   model number to start/restart simulation from. It would typically be used to restart a series of models from a specific model number, with the ``-n`` argument, e.g. to restart from A-scan 45 when creating a B-scan with 60 traces: ``(gprMax)$ python -m gprMax user_models/cylinder_Bscan_2D.in -n 15 -restart 45``
``-task``              integer   task identifier (model number) when running simulation as a job array on `Open Grid Scheduler/Grid Engine `_. For further details see the `parallel performance section of the User Guide `_
``-mpi``               integer   number of Message Passing Interface (MPI) tasks, i.e. master + workers, for MPI task farm. This option is most usefully combined with ``-n`` to allow individual models to be farmed out using a MPI task farm, e.g. to create a B-scan with 60 traces and use MPI to farm out each trace: ``(gprMax)$ python -m gprMax user_models/cylinder_Bscan_2D.in -n 60 -mpi 61``. For further details see the `parallel performance section of the User Guide `_
``--mpi-no-spawn``     flag      use MPI task farm without spawn mechanism. For further details see the `parallel performance section of the User Guide `_
``-benchmark``         flag      switch on benchmarking mode. This can be used to benchmark the threading (parallel) performance of gprMax on different hardware. For further details see the `benchmarking section of the User Guide `_
``--geometry-only``    flag      build a model and produce any geometry views but do not run the simulation, e.g. to check the geometry of a model is correct: ``(gprMax)$ python -m gprMax user_models/heterogeneous_soil.in --geometry-only``
``--geometry-fixed``   flag      run a series of models where the geometry does not change between models, e.g. a B-scan where *only* the position of simple sources and receivers, moved using ``#src_steps`` and ``#rx_steps``, changes between models.
``--opt-taguchi``      flag      run a series of models using an optimisation process based on Taguchi's method. For further details see the `user libraries section of the User Guide `_
``--write-processed``  flag      write another input file after any Python code and include commands in the original input file have been processed. Useful for checking that any Python code is being correctly processed into gprMax commands.
``-h`` or ``--help``   flag      used to get help on command line options.
====================== ========= ===========

"""^^xsd:string,
        """(gprMax)$ python setup.py install 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/gprMax/gprMax/> ;
    sd:keywords "antenna"^^xsd:string,
        "cuda"^^xsd:string,
        "electromagnetic"^^xsd:string,
        "fdtd"^^xsd:string,
        "gpr"^^xsd:string,
        "gpu"^^xsd:string,
        "modelling"^^xsd:string,
        "nvidia"^^xsd:string,
        "simulation"^^xsd:string,
        "soil"^^xsd:string ;
    sd:license "https://api.github.com/licenses/gpl-3.0"^^xsd:anyURI ;
    sd:name "gprMax/gprMax"^^xsd:string .

<https://example.org/objects/Software/haoliangyu/node-qa-masker/> a sd:Software ;
    sd:author <https://example.org/objects/Person/haoliangyu> ;
    sd:dateCreated "2016-08-07T06:34:16+00:00"^^xsd:dateTime ;
    sd:dateModified "2019-02-13T03:20:09+00:00"^^xsd:dateTime ;
    sd:description "generate Lansat 8 image and MODIS land product masks in Node.js"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/haoliangyu/node-qa-masker/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """``` bash
npm install qa-masker
```

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/haoliangyu/node-qa-masker/> ;
    sd:keywords "landsat"^^xsd:string,
        "mask"^^xsd:string,
        "modis-land-products"^^xsd:string,
        "nodejs"^^xsd:string ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "haoliangyu/node-qa-masker"^^xsd:string .

<https://example.org/objects/Software/harismuneer/Ultimate-Facebook-Scraper/> a sd:Software ;
    sd:author <https://example.org/objects/Person/harismuneer> ;
    sd:citation """  


If you use this tool for your research, then kindly cite it. Click the above badge for more information regarding the complete citation for this tool and diffferent citation formats like IEEE, APA etc.

---

"""^^xsd:string ;
    sd:dateCreated "2018-08-22T21:11:36+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T18:06:09+00:00"^^xsd:dateTime ;
    sd:description "🤖 A bot which scrapes almost everything about a Facebook user's profile including all public posts/statuses available on the user's timeline, uploaded photos, tagged photos, videos, friends list and their profile photos (including Followers, Following, Work Friends, College Friends etc)."^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/harismuneer/Ultimate-Facebook-Scraper/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """You will need to:

- Install latest version of [Google Chrome](https://www.google.com/chrome/).
- Install [Python 3](https://www.python.org/downloads/)
- Have a Facebook account without 2FA enabled

```bash
git clone https://github.com/harismuneer/Ultimate-Facebook-Scraper.git
cd Ultimate-Facebook-Scraper

#: Install Python requirements
pip install -e .
```

The code is multi-platform and is tested on both Windows and Linux.
Chrome driver is automatically downloaded using the chromedriver_manager package.

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/harismuneer/Ultimate-Facebook-Scraper/> ;
    sd:keywords "automated-scraper"^^xsd:string,
        "best-facebook-scraper"^^xsd:string,
        "facebook-connections-scrap"^^xsd:string,
        "facebook-crawler"^^xsd:string,
        "facebook-data-download"^^xsd:string,
        "facebook-data-scraper"^^xsd:string,
        "facebook-friends-scraper"^^xsd:string,
        "facebook-login"^^xsd:string,
        "facebook-photos-downloader"^^xsd:string,
        "facebook-pictures-downloader"^^xsd:string,
        "facebook-pictures-scraper"^^xsd:string,
        "facebook-profile-scraper"^^xsd:string,
        "facebook-scraper"^^xsd:string,
        "facebook-scraper-software"^^xsd:string,
        "facebook-scraper-tool"^^xsd:string,
        "facebook-scraping"^^xsd:string,
        "facebook-scrapper"^^xsd:string,
        "facebook-status-scraper"^^xsd:string,
        "facebook-timeline-scraper"^^xsd:string,
        "selenium-scraper"^^xsd:string ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "harismuneer/Ultimate-Facebook-Scraper"^^xsd:string .

<https://example.org/objects/Software/hezhangsprinter/DCPDN/> a sd:Software ;
    sd:author <https://example.org/objects/Person/hezhangsprinter> ;
    sd:citation """  year={2018} 
"""^^xsd:string,
        """@inproceedings{dehaze_zhang_2018,       
  title={Densely Connected Pyramid Dehazing Network},
  author={Zhang, He and Patel, Vishal M},
  booktitle={CVPR},
  year={2018}
}"""^^xsd:string,
        """@inproceedings{dehaze_zhang_2018,        
"""^^xsd:string,
        """Great thanks for the insight discussion with [Vishwanath Sindagi](http://www.vishwanathsindagi.com/) and initial discussion with [Dr. Kevin S. Zhou](https://sites.google.com/site/skevinzhou/home)

This work is under MIT license.
"""^^xsd:string ;
    sd:dateCreated "2018-03-08T21:53:34+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-01T13:13:21+00:00"^^xsd:dateTime ;
    sd:description "Densely Connected Pyramid Dehazing Network (CVPR'2018)"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/hezhangsprinter/DCPDN/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """   (conda install pytorch torchvision -c pytorch) 
"""^^xsd:string,
        """python setup.py install 
Install python package:  
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/hezhangsprinter/DCPDN/> ;
    sd:keywords "cnn"^^xsd:string,
        "cvpr2018"^^xsd:string,
        "dehazing"^^xsd:string,
        "gan"^^xsd:string ;
    sd:name "hezhangsprinter/DCPDN"^^xsd:string .

<https://example.org/objects/Software/hezhangsprinter/DID-MDN/> a sd:Software ;
    sd:author <https://example.org/objects/Person/hezhangsprinter> ;
    sd:citation """  year={2018} 
"""^^xsd:string,
        """@inproceedings{derain_zhang_2018,       
  title={Density-aware Single Image De-raining using a Multi-stream Dense Network},
  author={Zhang, He and Patel, Vishal M},
  booktitle={CVPR},
  year={2018}
}"""^^xsd:string,
        """Great thanks for the insight discussion with [Vishwanath Sindagi](http://www.vishwanathsindagi.com/) and help from [Hang Zhang](http://hangzh.com/)
"""^^xsd:string ;
    sd:dateCreated "2018-02-21T00:14:52+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-14T01:02:14+00:00"^^xsd:dateTime ;
    sd:description "Density-aware Single Image De-raining using a Multi-stream Dense Network  (CVPR 2018)"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/hezhangsprinter/DID-MDN/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """   (conda install pytorch torchvision -c pytorch) 
"""^^xsd:string,
        """   python setup.py install) 
Install python package:  
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/hezhangsprinter/DID-MDN/> ;
    sd:keywords "cnn"^^xsd:string,
        "cvpr2018"^^xsd:string,
        "dense"^^xsd:string,
        "derain"^^xsd:string ;
    sd:name "hezhangsprinter/DID-MDN"^^xsd:string .

<https://example.org/objects/Software/hiroharu-kato/neural_renderer/> a sd:Software ;
    sd:author <https://example.org/objects/Person/hiroharu-kato> ;
    sd:citation """@InProceedings{kato2018renderer
    title={Neural 3D Mesh Renderer},
    author={Kato, Hiroharu and Ushiku, Yoshitaka and Harada, Tatsuya},
    booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2018}
}"""^^xsd:string,
        """```
@InProceedings{kato2018renderer
    title={Neural 3D Mesh Renderer},
    author={Kato, Hiroharu and Ushiku, Yoshitaka and Harada, Tatsuya},
    booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2018}
}
```
"""^^xsd:string ;
    sd:dateCreated "2017-11-22T05:35:00+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T14:13:40+00:00"^^xsd:dateTime ;
    sd:description "\"Neural 3D Mesh Renderer\" (CVPR 2018) by H. Kato, Y. Ushiku, and T. Harada."^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/hiroharu-kato/neural_renderer/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """```
sudo python setup.py install
```

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/hiroharu-kato/neural_renderer/> ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "hiroharu-kato/neural_renderer"^^xsd:string .

<https://example.org/objects/Software/iannesbitt/readgssi/> a sd:Software ;
    sd:author <https://example.org/objects/Person/iannesbitt> ;
    sd:citation """Ian M. Nesbitt, François-Xavier Simon, Thomas Paulin, 2018. readgssi - an open-source tool to read and plot GSSI ground-penetrating radar data. [doi:10.5281/zenodo.1439119](https://dx.doi.org/10.5281/zenodo.1439119)

"""^^xsd:string ;
    sd:dateCreated "2017-11-11T16:35:38+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-04-27T20:11:15+00:00"^^xsd:dateTime ;
    sd:description "python tool to read and plot Geophysical Survey Systems Incorporated (GSSI) radar data"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/iannesbitt/readgssi/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """If you choose to install a specific commit rather than the [latest working release of this software](https://pypi.org/project/readgssi), you may download this package, unzip to your home folder, open a command line, then install in the following way:

```bash
pip install ~/readgssi
```

"""^^xsd:string,
        """Once you have [anaconda](https://www.anaconda.com/download) running, installing requirements is pretty easy.

```bash
conda config --add channels conda-forge
conda create -n readgssi python==3.7 pandas h5py pytz obspy
conda activate readgssi
pip install readgssi
```

That should allow you to run the commands below.

"""^^xsd:string,
        """You can make the command even more specific by further modifying the set of files returned by the `ls` command. For example:

```bash
for f in `ls FILE__{010..025}.DZT`; do readgssi -p 8 -n -r 0 -g 40 -Z 233 -z ns -N -x m -s auto -i $f; done
```

This command will process only the 16 files in the numeric sequence between and including `010` and `025` in the set (`FILE__010.DZT`, `FILE__011.DZT`, `...`, `FILE__025.DZT`). `bash` handles the zero padding for you as well. Pretty cool. 


"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/iannesbitt/readgssi/> ;
    sd:keywords "dzt"^^xsd:string,
        "georadar"^^xsd:string,
        "gpr"^^xsd:string,
        "ground-penetrating-radar"^^xsd:string,
        "gssi"^^xsd:string,
        "hdf5"^^xsd:string,
        "plot"^^xsd:string,
        "python2"^^xsd:string,
        "python3"^^xsd:string ;
    sd:license "https://api.github.com/licenses/agpl-3.0"^^xsd:anyURI ;
    sd:name "iannesbitt/readgssi"^^xsd:string .

<https://example.org/objects/Software/imfunniee/gitfolio/> a sd:Software ;
    sd:author <https://example.org/objects/Person/imfunniee> ;
    sd:dateCreated "2019-05-06T16:50:05+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T09:27:00+00:00"^^xsd:dateTime ;
    sd:description ":octocat: personal website + blog  for every github user"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/imfunniee/gitfolio/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """Install gitfolio

```sh
npm i gitfolio -g
```

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/imfunniee/gitfolio/> ;
    sd:keywords "awesome"^^xsd:string,
        "blog"^^xsd:string,
        "github"^^xsd:string,
        "node"^^xsd:string,
        "personal-website"^^xsd:string,
        "showcase"^^xsd:string,
        "theme"^^xsd:string ;
    sd:license "https://api.github.com/licenses/gpl-3.0"^^xsd:anyURI ;
    sd:name "imfunniee/gitfolio"^^xsd:string .

<https://example.org/objects/Software/joferkington/mplstereonet/> a sd:Software ;
    sd:author <https://example.org/objects/Person/joferkington> ;
    sd:dateCreated "2012-04-13T17:16:28+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-29T06:14:41+00:00"^^xsd:dateTime ;
    sd:description "Stereonets for matplotlib"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/joferkington/mplstereonet/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """pip install mplstereonet 
"""^^xsd:string,
        """python setup.py install 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/joferkington/mplstereonet/> ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "joferkington/mplstereonet"^^xsd:string .

<https://example.org/objects/Software/jupyter-widgets/ipyleaflet/> a sd:Software ;
    sd:author <https://example.org/objects/Person/jupyter-widgets> ;
    sd:dateCreated "2014-05-07T16:32:10+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T10:30:18+00:00"^^xsd:dateTime ;
    sd:description "A Jupyter - Leaflet.js bridge"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/jupyter-widgets/ipyleaflet/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """For a development installation (requires npm, you can install it with `conda install -c conda-forge nodejs`):

```
git clone https://github.com/jupyter-widgets/ipyleaflet.git
cd ipyleaflet
pip install -e .
```

If you are using the classic Jupyter Notebook you need to install the nbextension:

```
jupyter nbextension install --py --symlink --sys-prefix ipyleaflet
jupyter nbextension enable --py --sys-prefix ipyleaflet
```

If you are using JupyterLab, you need to install the labextension for ipywidgets and ipyleaflet:

```
jupyter labextension install @jupyter-widgets/jupyterlab-manager js
```

Note for developers:

- the ``-e`` pip option allows one to modify the Python code in-place. Restart the kernel in order to see the changes.
- the ``--symlink`` argument on Linux or OS X allows one to modify the JavaScript code in-place. This feature is not available with Windows.

    For automatically building the JavaScript code every time there is a change, run the following command from the ``ipyleaflet/js/`` directory:

    ```
    npm run watch
    ```

    If you are on JupyterLab you also need to run the following in a separate terminal:

    ```
    jupyter lab --watch
    ```

    Every time a JavaScript build has terminated you need to refresh the Notebook page in order to load the JavaScript code again.

"""^^xsd:string,
        """Using conda:

```
conda install -c conda-forge ipyleaflet
```

Using pip:

```
pip install ipyleaflet
```

If you are using the classic Jupyter Notebook < 5.3 you need to run this extra command:

```
jupyter nbextension enable --py --sys-prefix ipyleaflet
```

If you are using JupyterLab, you will need to install the JupyterLab extension:

```
jupyter labextension install @jupyter-widgets/jupyterlab-manager jupyter-leaflet
```

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/jupyter-widgets/ipyleaflet/> ;
    sd:keywords "jupyter"^^xsd:string,
        "jupyterlab-extension"^^xsd:string,
        "leaflet"^^xsd:string,
        "visualization"^^xsd:string ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "jupyter-widgets/ipyleaflet"^^xsd:string .

<https://example.org/objects/Software/jwass/mplleaflet/> a sd:Software ;
    sd:author <https://example.org/objects/Person/jwass> ;
    sd:dateCreated "2014-05-04T16:01:48+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-01T15:30:21+00:00"^^xsd:dateTime ;
    sd:description "Easily convert matplotlib plots from Python into interactive Leaflet web maps."^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/jwass/mplleaflet/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """$ pip install -e . 
"""^^xsd:string,
        """Install `mplleaflet` from PyPI using `$ pip install mplleaflet`.

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/jwass/mplleaflet/> ;
    sd:license "https://api.github.com/licenses/bsd-3-clause"^^xsd:anyURI ;
    sd:name "jwass/mplleaflet"^^xsd:string .

<https://example.org/objects/Software/kinverarity1/lasio/> a sd:Software ;
    sd:author <https://example.org/objects/Person/kinverarity1> ;
    sd:dateCreated "2013-12-24T08:56:49+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-31T03:03:50+00:00"^^xsd:dateTime ;
    sd:description """For the minimum working requirements, you'll need numpy installed. Install
lasio with:

```bash
$ pip install lasio
```

To make sure you have everything, use this to ensure pandas, cchardet, and
openpyxl are also installed:

```bash
$ pip install lasio[all]
```

Example session:

```python
>>> import lasio
```

You can read the file using a filename, file-like object, or URL:

```python
>>> las = lasio.read("sample_rev.las")
```

Data is accessible both directly as numpy arrays

```python
>>> las.keys()
['DEPT', 'DT', 'RHOB', 'NPHI', 'SFLU', 'SFLA', 'ILM', 'ILD']
>>> las['SFLU']
array([ 123.45,  123.45,  123.45, ...,  123.45,  123.45,  123.45])
>>> las['DEPT']
array([ 1670.   ,  1669.875,  1669.75 , ...,  1669.75 ,  1670.   ,
        1669.875])
```

and as ``CurveItem`` objects with associated metadata:

```python
>>> las.curves
[CurveItem(mnemonic=DEPT, unit=M, value=, descr=1  DEPTH, original_mnemonic=DEPT, data.shape=(29897,)),
CurveItem(mnemonic=DT, unit=US/M, value=, descr=2  SONIC TRANSIT TIME, original_mnemonic=DT, data.shape=(29897,)),
CurveItem(mnemonic=RHOB, unit=K/M3, value=, descr=3  BULK DENSITY, original_mnemonic=RHOB, data.shape=(29897,)),
CurveItem(mnemonic=NPHI, unit=V/V, value=, descr=4   NEUTRON POROSITY, original_mnemonic=NPHI, data.shape=(29897,)),
CurveItem(mnemonic=SFLU, unit=OHMM, value=, descr=5  RXO RESISTIVITY, original_mnemonic=SFLU, data.shape=(29897,)),
CurveItem(mnemonic=SFLA, unit=OHMM, value=, descr=6  SHALLOW RESISTIVITY, original_mnemonic=SFLA, data.shape=(29897,)),
CurveItem(mnemonic=ILM, unit=OHMM, value=, descr=7  MEDIUM RESISTIVITY, original_mnemonic=ILM, data.shape=(29897,)),
CurveItem(mnemonic=ILD, unit=OHMM, value=, descr=8  DEEP RESISTIVITY, original_mnemonic=ILD, data.shape=(29897,))]
```

Header information is parsed into simple HeaderItem objects, and stored in a
dictionary for each section of the header:

```python
>>> las.version
[HeaderItem(mnemonic=VERS, unit=, value=1.2, descr=CWLS LOG ASCII STANDARD -VERSION 1.2, original_mnemonic=VERS),
HeaderItem(mnemonic=WRAP, unit=, value=NO, descr=ONE LINE PER DEPTH STEP, original_mnemonic=WRAP)]
>>> las.well
[HeaderItem(mnemonic=STRT, unit=M, value=1670.0, descr=, original_mnemonic=STRT),
HeaderItem(mnemonic=STOP, unit=M, value=1660.0, descr=, original_mnemonic=STOP),
HeaderItem(mnemonic=STEP, unit=M, value=-0.125, descr=, original_mnemonic=STEP),
HeaderItem(mnemonic=NULL, unit=, value=-999.25, descr=, original_mnemonic=NULL),
HeaderItem(mnemonic=COMP, unit=, value=ANY OIL COMPANY LTD., descr=COMPANY, original_mnemonic=COMP),
HeaderItem(mnemonic=WELL, unit=, value=ANY ET AL OIL WELL #:12, descr=WELL, original_mnemonic=WELL),
HeaderItem(mnemonic=FLD, unit=, value=EDAM, descr=FIELD, original_mnemonic=FLD),
HeaderItem(mnemonic=LOC, unit=, value=A9-16-49, descr=LOCATION, original_mnemonic=LOC),
HeaderItem(mnemonic=PROV, unit=, value=SASKATCHEWAN, descr=PROVINCE, original_mnemonic=PROV),
HeaderItem(mnemonic=SRVC, unit=, value=ANY LOGGING COMPANY LTD., descr=SERVICE COMPANY, original_mnemonic=SRVC),
HeaderItem(mnemonic=DATE, unit=, value=25-DEC-1988, descr=LOG DATE, original_mnemonic=DATE),
HeaderItem(mnemonic=UWI, unit=, value=100091604920, descr=UNIQUE WELL ID, original_mnemonic=UWI)]
>>> las.params
[HeaderItem(mnemonic=BHT, unit=DEGC, value=35.5, descr=BOTTOM HOLE TEMPERATURE, original_mnemonic=BHT),
HeaderItem(mnemonic=BS, unit=MM, value=200.0, descr=BIT SIZE, original_mnemonic=BS),
HeaderItem(mnemonic=FD, unit=K/M3, value=1000.0, descr=FLUID DENSITY, original_mnemonic=FD),
HeaderItem(mnemonic=MATR, unit=, value=0.0, descr=NEUTRON MATRIX(0=LIME,1=SAND,2=DOLO), original_mnemonic=MATR),
HeaderItem(mnemonic=MDEN, unit=, value=2710.0, descr=LOGGING MATRIX DENSITY, original_mnemonic=MDEN),
HeaderItem(mnemonic=RMF, unit=OHMM, value=0.216, descr=MUD FILTRATE RESISTIVITY, original_mnemonic=RMF),
HeaderItem(mnemonic=DFD, unit=K/M3, value=1525.0, descr=DRILL FLUID DENSITY, original_mnemonic=DFD)]
```

The data is stored as a 2D numpy array:

```python
>>> las.data
array([[ 1670.   ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1669.875,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1669.75 ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       ...,
       [ 1669.75 ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1670.   ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1669.875,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ]])
```

You can also retrieve and load data as a ``pandas`` DataFrame, build LAS files
from scratch, write them back to disc, and export to Excel, amongst other
things.

See the [package documentation](https://lasio.readthedocs.io/en/latest/) for
more details.

"""^^xsd:string,
        "Python library for reading and writing well data using Log ASCII Standard (LAS) files"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/kinverarity1/lasio/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/kinverarity1/lasio/> ;
    sd:keywords "data-format"^^xsd:string,
        "data-management"^^xsd:string,
        "data-mining"^^xsd:string,
        "geology"^^xsd:string,
        "geophysics"^^xsd:string,
        "geotechnical-engineering"^^xsd:string,
        "groundwater"^^xsd:string,
        "io"^^xsd:string,
        "las-files"^^xsd:string,
        "mineral-exploration"^^xsd:string,
        "petroleum"^^xsd:string ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "kinverarity1/lasio"^^xsd:string .

<https://example.org/objects/Software/kosmtik/kosmtik/> a sd:Software ;
    sd:author <https://example.org/objects/Person/kosmtik> ;
    sd:dateCreated "2014-09-22T11:03:33+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-31T14:49:07+00:00"^^xsd:dateTime ;
    sd:description "Make maps with OpenStreetMap and Mapnik"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/kosmtik/kosmtik/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """Note: Node.js versions are moving very fast, and kosmtik or its dependencies are
hardly totally up to date with latest release. Ideally, you should run the LTS
version of Node.js. You can use a Node.js version manager (like
[NVM](https://github.com/creationix/nvm)) to help.

    npm -g install kosmtik

This might need root/Administrator rights. If you cannot install globally
you can also install locally with

    npm install kosmtik

This will create a `node_modules/kosmtik` folder. You then have to replace all occurences of `kosmtik`
below with `node node_modules/kosmtik/index.js`.

To reinstall all plugins:

    kosmtik plugins --reinstall

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/kosmtik/kosmtik/> ;
    sd:keywords "cartocss"^^xsd:string,
        "javascript"^^xsd:string,
        "kosmtik"^^xsd:string,
        "mapnik"^^xsd:string,
        "nodejs"^^xsd:string,
        "openstreetmap"^^xsd:string ;
    sd:license "None"^^xsd:anyURI ;
    sd:name "kosmtik/kosmtik"^^xsd:string .

<https://example.org/objects/Software/mapbox/geojson-vt/> a sd:Software ;
    sd:author <https://example.org/objects/Person/mapbox> ;
    sd:dateCreated "2014-12-03T04:25:50+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T03:54:36+00:00"^^xsd:dateTime ;
    sd:description "Slice GeoJSON into vector tiles on the fly in the browser"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/mapbox/geojson-vt/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """Install using NPM (`npm install geojson-vt`) or Yarn (`yarn add geojson-vt`), then:

```js
// import as a ES module
import geojsonvt from 'geojson-vt';

// or require in Node / Browserify
const geojsonvt = require('geojson-vt');
```

Or use a browser build directly:

```html

```
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/mapbox/geojson-vt/> ;
    sd:license "https://api.github.com/licenses/isc"^^xsd:anyURI ;
    sd:name "mapbox/geojson-vt"^^xsd:string .

<https://example.org/objects/Software/mapbox/rasterio/> a sd:Software ;
    sd:author <https://example.org/objects/Person/mapbox> ;
    sd:dateCreated "2013-11-04T16:36:27+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T17:26:51+00:00"^^xsd:dateTime ;
    sd:description """
Rasterio gives access to properties of a geospatial raster file.

.. code-block:: python

    with rasterio.open('tests/data/RGB.byte.tif') as src:
        print(src.width, src.height)
        print(src.crs)
        print(src.transform)
        print(src.count)
        print(src.indexes)

    # Printed:
    # (791, 718)
    # {u'units': u'm', u'no_defs': True, u'ellps': u'WGS84', u'proj': u'utm', u'zone': 18}
    # Affine(300.0379266750948, 0.0, 101985.0,
    #        0.0, -300.041782729805, 2826915.0)
    # 3
    # [1, 2, 3]

A rasterio dataset also provides methods for getting extended array slices given
georeferenced coordinates.


.. code-block:: python

    with rasterio.open('tests/data/RGB.byte.tif') as src:
        print src.window(**src.window_bounds(((100, 200), (100, 200))))

    # Printed:
    # ((100, 200), (100, 200))

"""^^xsd:string,
        "Rasterio reads and writes geospatial raster datasets"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/mapbox/rasterio/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """
The primary forum for questions about installation and usage of Rasterio is
https://rasterio.groups.io/g/main. The authors and other users will answer
questions when they have expertise to share and time to explain. Please take
the time to craft a clear question and be patient about responses.

Please do not bring these questions to Rasterio's issue tracker, which we want
to reserve for bug reports and other actionable issues.

While Rasterio's repo is in the Mapbox GitHub organization, Mapbox's Support
team is focused on customer support for its commercial platform and Rasterio
support requests may be perfunctorily closed with or without a link to
https://rasterio.groups.io/g/main. It's better to bring questions directly to
the main Rasterio group at groups.io.

"""^^xsd:string,
        """$ brew install gdal 
$ pip install -U pip 
$ pip install --no-binary rasterio 
Alternatively, you can install GDAL binaries from kyngchaos__.  You will then 
"""^^xsd:string,
        """$ conda install -c conda-forge rasterio 
"""^^xsd:string,
        """$ pip install -U pip 
"""^^xsd:string,
        """$ pip install -U pip 
$ pip install rasterio 
"""^^xsd:string,
        """$ python setup.py install 
"""^^xsd:string,
        """You can also install rasterio with conda using Anaconda's conda-forge channel. 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/mapbox/rasterio/> ;
    sd:keywords "cli"^^xsd:string,
        "cython"^^xsd:string,
        "gdal"^^xsd:string,
        "gis"^^xsd:string,
        "mapbox-satellite-oss"^^xsd:string,
        "python"^^xsd:string,
        "raster"^^xsd:string ;
    sd:license "None"^^xsd:anyURI ;
    sd:name "mapbox/rasterio"^^xsd:string .

<https://example.org/objects/Software/mapbox/tilelive-mapnik/> a sd:Software ;
    sd:author <https://example.org/objects/Person/mapbox> ;
    sd:dateCreated "2011-05-09T14:21:07+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-04-24T14:08:20+00:00"^^xsd:dateTime ;
    sd:description "mapnik renderer backend for tilelive"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/mapbox/tilelive-mapnik/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """    npm install tilelive-mapnik

Though `tilelive` is not a dependency of `tilelive-mapnik` you will want to
install it to actually make use of `tilelive-mapnik` through a reasonable
API.


"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/mapbox/tilelive-mapnik/> ;
    sd:license "https://api.github.com/licenses/bsd-3-clause"^^xsd:anyURI ;
    sd:name "mapbox/tilelive-mapnik"^^xsd:string .

<https://example.org/objects/Software/mapbox/tippecanoe/> a sd:Software ;
    sd:author <https://example.org/objects/Person/mapbox> ;
    sd:dateCreated "2014-09-26T22:16:45+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T15:22:49+00:00"^^xsd:dateTime ;
    sd:description "Build vector tilesets from large collections of GeoJSON features."^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/mapbox/tippecanoe/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """$ make install 
"""^^xsd:string,
        """C++ compiler or install prerequisite packages if you get 
"""^^xsd:string,
        """make install 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/mapbox/tippecanoe/> ;
    sd:keywords "c-plus-plus"^^xsd:string,
        "geojson"^^xsd:string,
        "vector-tiles"^^xsd:string ;
    sd:license "https://api.github.com/licenses/bsd-2-clause"^^xsd:anyURI ;
    sd:name "mapbox/tippecanoe"^^xsd:string .

<https://example.org/objects/Software/mbloch/mapshaper/> a sd:Software ;
    sd:author <https://example.org/objects/Person/mbloch> ;
    sd:dateCreated "2013-02-19T19:49:05+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T00:56:18+00:00"^^xsd:dateTime ;
    sd:description """Mapshaper is software for editing Shapefile, GeoJSON, [TopoJSON](https://github.com/mbostock/topojson/wiki), CSV and several other data formats, written in JavaScript.

Mapshaper supports essential map making tasks like simplifying shapes, editing attribute data, clipping, erasing, dissolving, filtering and more.

See the [project wiki](https://github.com/mbloch/mapshaper/wiki) for documentation on how to use mapshaper.

To suggest improvements, add an [issue](https://github.com/mbloch/mapshaper/issues).


"""^^xsd:string,
        "Tools for editing Shapefile, GeoJSON, TopoJSON and CSV files"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/mbloch/mapshaper/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions "npm run build     "^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/mbloch/mapshaper/> ;
    sd:keywords "csv"^^xsd:string,
        "geojson"^^xsd:string,
        "gis"^^xsd:string,
        "shapefile"^^xsd:string,
        "svg"^^xsd:string,
        "topojson"^^xsd:string ;
    sd:license "None"^^xsd:anyURI ;
    sd:name "mbloch/mapshaper"^^xsd:string .

<https://example.org/objects/Software/msracver/Flow-Guided-Feature-Aggregation/> a sd:Software ;
    sd:author <https://example.org/objects/Person/msracver> ;
    sd:citation """@inproceedings{dai16rfcn,
    Author = {Jifeng Dai, Yi Li, Kaiming He, Jian Sun},
    Title = {{R-FCN}: Object Detection via Region-based Fully Convolutional Networks},
    Conference = {NIPS},
    Year = {2016}
}"""^^xsd:string,
        """@inproceedings{zhu17fgfa,
    Author = {Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, Yichen Wei},
    Title = {Flow-Guided Feature Aggregation for Video Object Detection},
    Conference = {ICCV},
    Year = {2017}
}"""^^xsd:string,
        """If you find Flow-Guided Feature Aggregation useful in your research, please consider citing:
```
@inproceedings{zhu17fgfa,
    Author = {Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, Yichen Wei},
    Title = {Flow-Guided Feature Aggregation for Video Object Detection},
    Conference = {ICCV},
    Year = {2017}
}

@inproceedings{dai16rfcn,
    Author = {Jifeng Dai, Yi Li, Kaiming He, Jian Sun},
    Title = {{R-FCN}: Object Detection via Region-based Fully Convolutional Networks},
    Conference = {NIPS},
    Year = {2016}
}
```

"""^^xsd:string ;
    sd:dateCreated "2017-08-17T09:50:06+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-02T03:04:58+00:00"^^xsd:dateTime ;
    sd:description """**Flow-Guided Feature Aggregation (FGFA)** is initially described in an [ICCV 2017 paper](https://arxiv.org/abs/1703.10025). It provides an accurate and end-to-end learning framework for video object detection. The proposed FGFA method, together with our previous work of [Deep Feature Flow](https://github.com/msracver/Deep-Feature-Flow), powered the winning entry of [ImageNet VID 2017](http://image-net.org/challenges/LSVRC/2017/results). It is worth noting that:

* FGFA improves the per-frame features by aggregating nearby frame features along the motion paths. It significantly improves the object detection accuracy in videos, especially for fast moving objects.
* FGFA is end-to-end trainable for the task of video object detection, which is vital for improving the recognition accuracy.
* We proposed to evaluate the detection accuracy for slow, medium and fast moving objects respectively, for better understanding and analysis of video object detection. The [motion-specific evaluation code](lib/dataset/imagenet_vid_eval_motion.py) is included in this repository.

***Click image to watch our demo video***

[![Demo Video on YouTube](https://media.giphy.com/media/7D9tmDgzB10HK/giphy.gif)](https://www.youtube.com/watch?v=R2h3DbTPvVg)

***Example object instances with slow, medium and fast motions***

![Instance Motion](instance_motion.png)

"""^^xsd:string,
        "Flow-Guided Feature Aggregation for Video Object Detection"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/msracver/Flow-Guided-Feature-Aggregation/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """    pip install Cython 
    pip install opencv-python==3.2.0.6 
    pip install easydict==1.6 
"""^^xsd:string,
        """1. Clone the Flow-Guided Feature Aggregation repository, and we call the directory that you cloned as ${FGFA_ROOT}.

~~~
git clone https://github.com/msracver/Flow-Guided-Feature-Aggregation.git
~~~
2. For Windows users, run ``cmd .\\init.bat``. For Linux user, run `sh ./init.sh`. The scripts will build cython module automatically and create some folders.

3. Install MXNet:

	3.1 Clone MXNet and checkout to [MXNet@(v0.10.0)](https://github.com/apache/incubator-mxnet/tree/v0.10.0) by
	```
	git clone --recursive https://github.com/apache/incubator-mxnet.git
	cd incubator-mxnet
	git checkout v0.10.0
	git submodule update
	```
	3.2 Copy operators in `$(FGFA_ROOT)/fgfa_rfcn/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by
	```
	cp -r $(FGFA_ROOT)/fgfa_rfcn/operator_cxx/* $(MXNET_ROOT)/src/operator/contrib/
	```
	3.3 Compile MXNet
	```
	cd ${MXNET_ROOT}
	make -j4
	```
	3.4 Install the MXNet Python binding by

	***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***
	```
	cd python
	sudo python setup.py install
	```
	3.5 For advanced users, you may put your Python packge into `./external/mxnet/$(YOUR_MXNET_PACKAGE)`, and modify `MXNET_VERSION` in `./experiments/fgfa_rfcn/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`. Thus you can switch among different versions of MXNet quickly.


"""^^xsd:string,
        """1. Please download ILSVRC2015 DET and ILSVRC2015 VID dataset, and make sure it looks like this:

	```
	./data/ILSVRC2015/
	./data/ILSVRC2015/Annotations/DET
	./data/ILSVRC2015/Annotations/VID
	./data/ILSVRC2015/Data/DET
	./data/ILSVRC2015/Data/VID
	./data/ILSVRC2015/ImageSets
	```

2. Please download ImageNet pre-trained ResNet-v1-101 model and Flying-Chairs pre-trained FlowNet model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMOBdCBiNaKbcjPrA), and put it under folder `./model`. Make sure it looks like this:
	```
	./model/pretrained_model/resnet_v1_101-0000.params
	./model/pretrained_model/flownet-0000.params
	```

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/msracver/Flow-Guided-Feature-Aggregation/> ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "msracver/Flow-Guided-Feature-Aggregation"^^xsd:string .

<https://example.org/objects/Software/nextflow-io/nextflow/> a sd:Software ;
    sd:author <https://example.org/objects/Person/nextflow-io> ;
    sd:dateCreated "2013-03-27T11:17:33+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T18:48:25+00:00"^^xsd:dateTime ;
    sd:description """
Download the package
--------------------

Nextflow does not require any installation procedure, just download the distribution package by copying and pasting
this command in your terminal:

```
curl -fsSL https://get.nextflow.io | bash
```

It creates the ``nextflow`` executable file in the current directory. You may want to move it to a folder accessible from your ``$PATH``.

Download from Conda
-------------------

Nextflow can also be installed from Bioconda

```
conda install -c bioconda nextflow 
```

"""^^xsd:string,
        "A DSL for data-driven computational pipelines"^^xsd:string,
        """Nextflow is a bioinformatics workflow manager that enables the development of portable and reproducible workflows.
It supports deploying workflows on a variety of execution platforms including local, HPC schedulers, AWS Batch,
Google Cloud Life Sciences, and Kubernetes. Additionally, it provides support for manage your workflow dependencies
through built-in support for Conda, Docker, Singularity, and Modules.

## Contents
- [Rationale](#rationale)
- [Quick start](#quick-start)
- [Documentation](#documentation)
- [Tool Management](#tool-management)
  - [Conda environments](#conda-environments)
  - [Docker and Singularity](#containers)
  - [Environment Modules](#environment-modules)
- [HPC Schedulers](#hpc-schedulers)
  - [SGE](#hpc-schedulers)
  - [Univa Grid Engine](#hpc-schedulers)
  - [LSF](#hpc-schedulers)
  - [SLURM](#hpc-schedulers)
  - [PBS/Torque](#hpc-schedulers)
  - [HTCondor (experimental)](#hpc-schedulers)
- [Cloud Support](#cloud-support)
  - [AWS Batch](#cloud-support)
  - [AWS EC2](#cloud-support)
  - [Google Cloud](#cloud-support)
  - [Google Genomics Pipelines](#cloud-support)
  - [Kubernetes](#cloud-support)
- [Community](#community)
- [Build from source](#build-from-source)
- [Contributing](#contributing)
- [License](#license)
- [Citations](#citations)
- [Credits](#credits)


"""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/nextflow-io/nextflow/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """make install 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/nextflow-io/nextflow/> ;
    sd:keywords "aws"^^xsd:string,
        "bioinformatics"^^xsd:string,
        "cloud"^^xsd:string,
        "data-flow"^^xsd:string,
        "docker"^^xsd:string,
        "groovy"^^xsd:string,
        "hpc"^^xsd:string,
        "nextflow"^^xsd:string,
        "pipeline"^^xsd:string,
        "pipeline-framework"^^xsd:string,
        "reproducible-research"^^xsd:string,
        "reproducible-science"^^xsd:string,
        "sge"^^xsd:string,
        "singularity"^^xsd:string,
        "singularity-containers"^^xsd:string,
        "slurm"^^xsd:string,
        "workflow-engine"^^xsd:string ;
    sd:license "https://api.github.com/licenses/apache-2.0"^^xsd:anyURI ;
    sd:name "nextflow-io/nextflow"^^xsd:string .

<https://example.org/objects/Software/nypl-spacetime/map-vectorizer/> a sd:Software ;
    sd:author <https://example.org/objects/Person/nypl-spacetime> ;
    sd:dateCreated "2013-07-16T20:42:23+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-02-20T11:55:01+00:00"^^xsd:dateTime ;
    sd:description "An open-source map vectorizer"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/nypl-spacetime/map-vectorizer/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """    install.packages('alphahull') 
    install.packages('igraph') 
"""^^xsd:string,
        """If you use PIP (recommended) you will get the necessary Python packages with: pip install -r requirements.txt 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/nypl-spacetime/map-vectorizer/> ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "nypl-spacetime/map-vectorizer"^^xsd:string .

<https://example.org/objects/Software/odoe/generator-arcgis-js-app/> a sd:Software ;
    sd:author <https://example.org/objects/Person/odoe> ;
    sd:dateCreated "2015-07-19T17:14:15+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-01-18T13:49:10+00:00"^^xsd:dateTime ;
    sd:description "Generator for ArcGIS JS API applications"^^xsd:string,
        """Yeoman has a heart of gold. He's a person with feelings and opinions, but he's very easy to work with. If you think he's too opinionated, he can be easily convinced.

If you'd like to get to know Yeoman better and meet some of his friends, [Grunt](http://gruntjs.com) and [Bower](http://bower.io), check out the complete [Getting Started Guide](https://github.com/yeoman/yeoman/wiki/Getting-Started).


"""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/odoe/generator-arcgis-js-app/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """npm install -g bower 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/odoe/generator-arcgis-js-app/> ;
    sd:name "odoe/generator-arcgis-js-app"^^xsd:string .

<https://example.org/objects/Software/ondrolexa/apsg/> a sd:Software ;
    sd:author <https://example.org/objects/Person/ondrolexa> ;
    sd:dateCreated "2014-10-07T06:31:58+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-28T04:24:51+00:00"^^xsd:dateTime ;
    sd:description """APSG can be installed using pip:
```
pip install apsg
```
If you want tu run the latest version of code, you can install it from git:
```
pip install git+git://github.com/ondrolexa/apsg.git
```
Alternatively, you can download the package manually from the GitHub repository [https://github.com/ondrolexa/apsg](https://github.com/ondrolexa/apsg), unzip it, navigate into the package, and use the command:
```
python setup.py install
```
"""^^xsd:string,
        "Structural geology package for Python"^^xsd:string,
        """You can see APSG in action in accompanied Jupyter notebook [http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/apsg_tutorial.ipynb](http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/apsg_tutorial.ipynb)

And for fun check how simply you can animate stereonets
[http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/animation_example.ipynb](http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/animation_example.ipynb)

"""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/ondrolexa/apsg/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """In rare cases, users reported problems on certain systems with the default pip installation command, which installs APSG from the binary distribution ("wheels") on PyPI. If you should encounter similar problems, you could try to install APSG from the source distribution instead via
```
pip install --no-binary :all: apsg
```
Also, I would appreciate it if you could report any issues that occur when using `pip install apsg` in hope that we can fix these in future releases.

"""^^xsd:string,
        """Installing `apsg` from the `conda-forge` channel can be achieved by adding `conda-forge` to your channels with:

```
conda config --add channels conda-forge
```

Once the `conda-forge` channel has been enabled, `apsg` can be installed with:

```
conda install apsg
```

It is possible to list all of the versions of `apsg` available on your platform with:

```
conda search apsg --channel conda-forge
```

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/ondrolexa/apsg/> ;
    sd:keywords "python"^^xsd:string,
        "structural-geology"^^xsd:string ;
    sd:license "None"^^xsd:anyURI ;
    sd:name "ondrolexa/apsg"^^xsd:string .

<https://example.org/objects/Software/phoenix104104/LapSRN/> a sd:Software ;
    sd:author <https://example.org/objects/Person/phoenix104104> ;
    sd:citation """@inproceedings{LapSRN,
    author    = {Lai, Wei-Sheng and Huang, Jia-Bin and Ahuja, Narendra and Yang, Ming-Hsuan}, 
    title     = {Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution}, 
    booktitle = {IEEE Conferene on Computer Vision and Pattern Recognition},
    year      = {2017}
}"""^^xsd:string,
        """If you find the code and datasets useful in your research, please cite:
    
    @inproceedings{LapSRN,
        author    = {Lai, Wei-Sheng and Huang, Jia-Bin and Ahuja, Narendra and Yang, Ming-Hsuan}, 
        title     = {Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution}, 
        booktitle = {IEEE Conferene on Computer Vision and Pattern Recognition},
        year      = {2017}
    }
    

"""^^xsd:string ;
    sd:dateCreated "2017-04-06T11:25:05+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-31T19:51:19+00:00"^^xsd:dateTime ;
    sd:description """    $ matlab
    >> install
   
If you install MatConvNet in your own path, you need to change the corresponding path in `install.m`, `train_LapSRN.m` and `test_LapSRN.m`.

"""^^xsd:string,
        "Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution (CVPR 2017)"^^xsd:string,
        """The Laplacian Pyramid Super-Resolution Network (LapSRN) is a progressive super-resolution model that super-resolves an low-resolution images in a coarse-to-fine Laplacian pyramid framework.
Our method is fast and achieves state-of-the-art performance on five benchmark datasets for 4x and 8x SR.
For more details and evaluation results, please check out our [project webpage](http://vllab.ucmerced.edu/wlai24/LapSRN/) and [paper](http://vllab.ucmerced.edu/wlai24/LapSRN/papers/cvpr17_LapSRN.pdf).

![teaser](http://vllab.ucmerced.edu/wlai24/LapSRN/images/emma_text.gif)



"""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/phoenix104104/LapSRN/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """Download repository:

    $ git clone https://github.com/phoenix104104/LapSRN.git

Run install.m in MATLAB to compile MatConvNet:

    """^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/phoenix104104/LapSRN/> ;
    sd:name "phoenix104104/LapSRN"^^xsd:string .

<https://example.org/objects/Software/phuang17/DeepMVS/> a sd:Software ;
    sd:author <https://example.org/objects/Person/phuang17> ;
    sd:citation """  booktitle    = "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", 
  year         = "2018" 
"""^^xsd:string,
        """@inproceedings{DeepMVS,
  author       = "Huang, Po-Han and Matzen, Kevin and Kopf, Johannes and Ahuja, Narendra and Huang, Jia-Bin",
  title        = "DeepMVS: Learning Multi-View Stereopsis",
  booktitle    = "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
  year         = "2018"
}"""^^xsd:string ;
    sd:dateCreated "2018-02-07T03:13:56+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-26T08:26:57+00:00"^^xsd:dateTime ;
    sd:description "DeepMVS: Learning Multi-View Stereopsis"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/phuang17/DeepMVS/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/phuang17/DeepMVS/> ;
    sd:license "https://api.github.com/licenses/bsd-2-clause"^^xsd:anyURI ;
    sd:name "phuang17/DeepMVS"^^xsd:string .

<https://example.org/objects/Software/puppeteer/puppeteer/> a sd:Software ;
    sd:author <https://example.org/objects/Person/puppeteer> ;
    sd:dateCreated "2017-05-09T22:16:13+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T22:00:51+00:00"^^xsd:dateTime ;
    sd:description "Headless Chrome Node.js API"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/puppeteer/puppeteer/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """To use Puppeteer in your project, run:

```bash
npm i puppeteer
#: or "yarn add puppeteer"
```

Note: When you install Puppeteer, it downloads a recent version of Chromium (~170MB Mac, ~282MB Linux, ~280MB Win) that is guaranteed to work with the API. To skip the download, or to download a different browser, see [Environment variables](https://github.com/puppeteer/puppeteer/blob/v3.3.0/docs/api.md#environment-variables).


"""^^xsd:string,
        """We have a [troubleshooting](https://github.com/puppeteer/puppeteer/blob/master/docs/troubleshooting.md) guide for various operating systems that lists the required dependencies.

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/puppeteer/puppeteer/> ;
    sd:keywords "automation"^^xsd:string,
        "developer-tools"^^xsd:string,
        "headless-chrome"^^xsd:string,
        "node-module"^^xsd:string,
        "testing"^^xsd:string,
        "web"^^xsd:string ;
    sd:license "https://api.github.com/licenses/apache-2.0"^^xsd:anyURI ;
    sd:name "puppeteer/puppeteer"^^xsd:string .

<https://example.org/objects/Software/pyro-ppl/pyro/> a sd:Software ;
    sd:author <https://example.org/objects/Person/pyro-ppl> ;
    sd:citation """@article{bingham2019pyro,
  author    = {Eli Bingham and
               Jonathan P. Chen and
               Martin Jankowiak and
               Fritz Obermeyer and
               Neeraj Pradhan and
               Theofanis Karaletsos and
               Rohit Singh and
               Paul A. Szerlip and
               Paul Horsfall and
               Noah D. Goodman},
  title     = {Pyro: Deep Universal Probabilistic Programming},
  journal   = {J. Mach. Learn. Res.},
  volume    = {20},
  pages     = {28:1--28:6},
  year      = {2019},
  url       = {http://jmlr.org/papers/v20/18-403.html}
}"""^^xsd:string,
        """If you use Pyro, please consider citing:
```
@article{bingham2019pyro,
  author    = {Eli Bingham and
               Jonathan P. Chen and
               Martin Jankowiak and
               Fritz Obermeyer and
               Neeraj Pradhan and
               Theofanis Karaletsos and
               Rohit Singh and
               Paul A. Szerlip and
               Paul Horsfall and
               Noah D. Goodman},
  title     = {Pyro: Deep Universal Probabilistic Programming},
  journal   = {J. Mach. Learn. Res.},
  volume    = {20},
  pages     = {28:1--28:6},
  year      = {2019},
  url       = {http://jmlr.org/papers/v20/18-403.html}
}
```
"""^^xsd:string ;
    sd:dateCreated "2017-06-16T05:03:47+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T20:40:17+00:00"^^xsd:dateTime ;
    sd:description "Deep universal probabilistic programming with Python and PyTorch"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/pyro-ppl/pyro/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """**Install using pip:**

Pyro supports Python 3.4+.

```sh
pip install pyro-ppl
```

**Install from source:**
```sh
git clone git@github.com:pyro-ppl/pyro.git
cd pyro
git checkout master  #: master is pinned to the latest release
pip install .
```

**Install with extra packages:**

To install the dependencies required to run the probabilistic models included in the `examples`/`tutorials` directories, please use the following command:
```sh
pip install pyro-ppl[extras] 
```
Make sure that the models come from the same release version of the [Pyro source code](https://github.com/pyro-ppl/pyro/releases) as you have installed.

"""^^xsd:string,
        """For recent features you can install Pyro from source.

**Install using pip:**

```sh
pip install git+https://github.com/pyro-ppl/pyro.git
```

or, with the `extras` dependency to run the probabilistic models included in the `examples`/`tutorials` directories:
```sh
pip install git+https://github.com/pyro-ppl/pyro.git#:egg=project[extras]
```

**Install from source:**

```sh
git clone https://github.com/pyro-ppl/pyro
cd pyro
pip install .  #: pip install .[extras] for running models in examples/tutorials
```

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/pyro-ppl/pyro/> ;
    sd:keywords "bayesian"^^xsd:string,
        "bayesian-inference"^^xsd:string,
        "machine-learning"^^xsd:string,
        "probabilistic-modeling"^^xsd:string,
        "probabilistic-programming"^^xsd:string,
        "python"^^xsd:string,
        "pytorch"^^xsd:string,
        "variational-inference"^^xsd:string ;
    sd:license "https://api.github.com/licenses/apache-2.0"^^xsd:anyURI ;
    sd:name "pyro-ppl/pyro"^^xsd:string .

<https://example.org/objects/Software/pysal/pysal/> a sd:Software ;
    sd:author <https://example.org/objects/Person/pysal> ;
    sd:dateCreated "2013-02-19T17:27:42+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-02T14:42:33+00:00"^^xsd:dateTime ;
    sd:description """If you are interested in contributing to PySAL please see our [development guidelines](https://github.com/pysal/pysal/wiki).

"""^^xsd:string,
        "PySAL: Python Spatial Analysis Library Meta-Package"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/pysal/pysal/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """PySAL is available through [Anaconda](https://www.continuum.io/downloads) (in the defaults or conda-forge channel) We recommend installing PySAL from conda-forge:

``` {.sourceCode .bash}
conda config --add channels conda-forge
conda install pysal
```

PySAL can also be installed using pip:

``` {.sourceCode .bash}
pip install pysal
```

As of version 2.0.0 PySAL has shifted to Python 3 only.

Users who need an older stable version of PySAL that is Python 2 compatible can install version 1.14.3 through pip or conda:

``` {.sourceCode .bash}
conda install pysal==1.14.3
```

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/pysal/pysal/> ;
    sd:license "https://api.github.com/licenses/bsd-3-clause"^^xsd:anyURI ;
    sd:name "pysal/pysal"^^xsd:string .

<https://example.org/objects/Software/pyvista/pymeshfix/> a sd:Software ;
    sd:author <https://example.org/objects/Person/pyvista> ;
    sd:dateCreated "2016-09-20T15:59:08+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-21T17:53:47+00:00"^^xsd:dateTime ;
    sd:description "Python Wrapper for MeshFix: easily repair holes in PyVista surface meshes"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/pyvista/pymeshfix/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """If you can't or don't want to install vtk, you can install it without 
"""^^xsd:string,
        """pip install . 
"""^^xsd:string,
        """pip install pymeshfix 
"""^^xsd:string,
        """pip install pymeshfix --no-dependencies 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/pyvista/pymeshfix/> ;
    sd:keywords "3d"^^xsd:string,
        "3d-reconstruction"^^xsd:string,
        "mesh"^^xsd:string,
        "mesh-processing"^^xsd:string ;
    sd:license "https://api.github.com/licenses/gpl-3.0"^^xsd:anyURI ;
    sd:name "pyvista/pymeshfix"^^xsd:string .

<https://example.org/objects/Software/pyvista/pyvista/> a sd:Software ;
    sd:author <https://example.org/objects/Person/pyvista> ;
    sd:citation """@article{sullivan2019pyvista,
  doi = {10.21105/joss.01450},
  url = {https://doi.org/10.21105/joss.01450},
  year = {2019},
  month = {may},
  publisher = {The Open Journal},
  volume = {4},
  number = {37},
  pages = {1450},
  author = {C. Bane Sullivan and Alexander Kaszynski},
  title = {{PyVista}: 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit ({VTK})},
  journal = {Journal of Open Source Software}
}"""^^xsd:string ;
    sd:dateCreated "2017-05-31T18:01:42+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T13:57:55+00:00"^^xsd:dateTime ;
    sd:description "3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK)"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/pyvista/pyvista/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """
PyVista is a powerful tool that researchers can harness to create compelling,
integrated visualizations of large datasets in an intuitive, Pythonic manner.
Here are a few open-source projects that leverage PyVista:

* itkwidgets_: Interactive Jupyter widgets to visualize images, point sets, and meshes in 2D and 3D. Supports all PyVista mesh types.
* pyansys_: Pythonic interface to ANSYS result, full, and archive files
* PVGeo_: Python package of VTK-based algorithms to analyze geoscientific data and models. PyVista is used to make the inputs and outputs of PVGeo's algorithms more accessible.
* omfvista_: 3D visualization for the Open Mining Format (omf). PyVista provides the foundation for this library's visualization.
* discretize_: Discretization tools for finite volume and inverse problems. ``discretize`` provides ``toVTK`` methods that return PyVista versions of their data types for `creating compelling visualizations`_.
* pymeshfix_: Python/Cython wrapper of Marco Attene's wonderful, award-winning MeshFix software.
* tetgen_: Python Interface to Hang Si's C++ TetGen Library


.. _itkwidgets: https://github.com/InsightSoftwareConsortium/itkwidgets
.. _pyansys: https://github.com/akaszynski/pyansys
.. _PVGeo: https://github.com/OpenGeoVis/PVGeo
.. _omfvista: https://github.com/OpenGeoVis/omfvista
.. _discretize: http://discretize.simpeg.xyz/en/master/
.. _creating compelling visualizations: http://discretize.simpeg.xyz/en/master/api/generated/discretize.mixins.vtkModule.html
.. _pymeshfix: https://github.com/pyvista/pymeshfix
.. _MeshFix: https://github.com/MarcoAttene/MeshFix-V2.1
.. _tetgen: https://github.com/pyvista/tetgen


"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/pyvista/pyvista/> ;
    sd:keywords "3d"^^xsd:string,
        "mesh"^^xsd:string,
        "mesh-processing"^^xsd:string,
        "meshviewer"^^xsd:string,
        "open-science"^^xsd:string,
        "plotting"^^xsd:string,
        "python"^^xsd:string,
        "scientific-research"^^xsd:string,
        "scientific-visualization"^^xsd:string,
        "visualization"^^xsd:string,
        "vtk"^^xsd:string ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "pyvista/pyvista"^^xsd:string .

<https://example.org/objects/Software/pyvista/tetgen/> a sd:Software ;
    sd:author <https://example.org/objects/Person/pyvista> ;
    sd:dateCreated "2018-06-16T10:04:57+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-30T10:17:42+00:00"^^xsd:dateTime ;
    sd:description "A Python interface to the C++ TetGen library to generate tetrahedral meshes of any 3D polyhedral domains"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/pyvista/tetgen/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """pip install . 
"""^^xsd:string,
        """pip install tetgen 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/pyvista/tetgen/> ;
    sd:keywords "3d"^^xsd:string,
        "mesh"^^xsd:string,
        "mesh-generation"^^xsd:string,
        "tetrahedral-meshing"^^xsd:string ;
    sd:license "None"^^xsd:anyURI ;
    sd:name "pyvista/tetgen"^^xsd:string .

<https://example.org/objects/Software/reduxjs/react-redux/> a sd:Software ;
    sd:author <https://example.org/objects/Person/reduxjs> ;
    sd:dateCreated "2015-07-11T17:32:01+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T22:07:42+00:00"^^xsd:dateTime ;
    sd:description "Official React bindings for Redux"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/reduxjs/react-redux/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """npm install react-redux 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/reduxjs/react-redux/> ;
    sd:keywords "react"^^xsd:string,
        "redux"^^xsd:string ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "reduxjs/react-redux"^^xsd:string .

<https://example.org/objects/Software/rowanz/neural-motifs/> a sd:Software ;
    sd:author <https://example.org/objects/Person/rowanz> ;
    sd:citation """  booktitle = "Conference on Computer Vision and Pattern Recognition",   
  year={2018} 
"""^^xsd:string,
        """@inproceedings{zellers2018scenegraphs,
  title={Neural Motifs: Scene Graph Parsing with Global Context},
  author={Zellers, Rowan and Yatskar, Mark and Thomson, Sam and Choi, Yejin},
  booktitle = "Conference on Computer Vision and Pattern Recognition",  
  year={2018}
}"""^^xsd:string ;
    sd:dateCreated "2018-03-08T23:50:52+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-30T04:47:34+00:00"^^xsd:dateTime ;
    sd:description "Code for Neural Motifs: Scene Graph Parsing with Global Context (CVPR 2018)"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/rowanz/neural-motifs/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """0. Install python3.6 and pytorch 3. I recommend the [Anaconda distribution](https://repo.continuum.io/archive/). To install PyTorch if you haven't already, use
 ```conda install pytorch=0.3.0 torchvision=0.2.0 cuda90 -c pytorch```.
 
1. Update the config file with the dataset paths. Specifically:
    - Visual Genome (the VG_100K folder, image_data.json, VG-SGG.h5, and VG-SGG-dicts.json). See data/stanford_filtered/README.md for the steps I used to download these.
    - You'll also need to fix your PYTHONPATH: ```export PYTHONPATH=/home/rowan/code/scene-graph``` 

2. Compile everything. run ```make``` in the main directory: this compiles the Bilinear Interpolation operation for the RoIs as well as the Highway LSTM.

3. Pretrain VG detection. The old version involved pretraining COCO as well, but we got rid of that for simplicity. Run ./scripts/pretrain_detector.sh
Note: You might have to modify the learning rate and batch size, particularly if you don't have 3 Titan X GPUs (which is what I used). [You can also download the pretrained detector checkpoint here.](https://drive.google.com/open?id=11zKRr2OF5oclFL47kjFYBOxScotQzArX)

4. Train VG scene graph classification: run ./scripts/train_models_sgcls.sh 2 (will run on GPU 2). OR, download the MotifNet-cls checkpoint here: [Motifnet-SGCls/PredCls](https://drive.google.com/open?id=12qziGKYjFD3LAnoy4zDT3bcg5QLC0qN6).
5. Refine for detection: run ./scripts/refine_for_detection.sh 2 or download the [Motifnet-SGDet](https://drive.google.com/open?id=1thd_5uSamJQaXAPVGVOUZGAOfGCYZYmb) checkpoint.
6. Evaluate: Refer to the scripts ./scripts/eval_models_sg[cls/det].sh.

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/rowanz/neural-motifs/> ;
    sd:keywords "pytorch"^^xsd:string,
        "scene-graph"^^xsd:string,
        "vision"^^xsd:string,
        "visual-genome"^^xsd:string ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "rowanz/neural-motifs"^^xsd:string .

<https://example.org/objects/Software/salihkaragoz/pose-residual-network-pytorch/> a sd:Software ;
    sd:author <https://example.org/objects/Person/salihkaragoz> ;
    sd:citation """@Inproceedings{kocabas18prn,
  Title          = {Multi{P}ose{N}et: Fast Multi-Person Pose Estimation using Pose Residual Network},
  Author         = {Kocabas, Muhammed and Karagoz, Salih and Akbas, Emre},
  Booktitle      = {European Conference on Computer Vision (ECCV)},
  Year           = {2018}
}"""^^xsd:string,
        """If you find this code useful for your research, please consider citing our paper:
```
@Inproceedings{kocabas18prn,
  Title          = {Multi{P}ose{N}et: Fast Multi-Person Pose Estimation using Pose Residual Network},
  Author         = {Kocabas, Muhammed and Karagoz, Salih and Akbas, Emre},
  Booktitle      = {European Conference on Computer Vision (ECCV)},
  Year           = {2018}
}
```
"""^^xsd:string ;
    sd:dateCreated "2018-07-18T10:49:53+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-02T12:44:58+00:00"^^xsd:dateTime ;
    sd:description "Code for the Pose Residual Network introduced in 'MultiPoseNet: Fast Multi-Person  Pose Estimation using Pose Residual Network' paper https://arxiv.org/abs/1807.04067"^^xsd:string,
        """We have tested our method on [Coco Dataset](http://cocodataset.org)

"""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/salihkaragoz/pose-residual-network-pytorch/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """1. Clone this repository 
`git clone https://github.com/salihkaragoz/pose-residual-network-pytorch.git`

2. Install [Pytorch](https://pytorch.org/)

3. `pip install -r src/requirements.txt`

4. To download COCO dataset train2017 and val2017 annotations run: `bash data/coco.sh`. (data size: ~240Mb)

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/salihkaragoz/pose-residual-network-pytorch/> ;
    sd:keywords "deep-neural-networks"^^xsd:string,
        "human-behavior-understanding"^^xsd:string,
        "human-pose-estimation"^^xsd:string,
        "pose-estimation"^^xsd:string,
        "python"^^xsd:string,
        "pytorch"^^xsd:string ;
    sd:name "salihkaragoz/pose-residual-network-pytorch"^^xsd:string .

<https://example.org/objects/Software/scikit-image/scikit-image/> a sd:Software ;
    sd:author <https://example.org/objects/Person/scikit-image> ;
    sd:citation """If you find this project useful, please cite:

> Stéfan van der Walt, Johannes L. Schönberger, Juan Nunez-Iglesias,
> François Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle
> Gouillart, Tony Yu, and the scikit-image contributors.
> *scikit-image: Image processing in Python*. PeerJ 2:e453 (2014)
> https://doi.org/10.7717/peerj.453
"""^^xsd:string ;
    sd:dateCreated "2011-07-07T22:07:20+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T18:13:39+00:00"^^xsd:dateTime ;
    sd:description "Image processing in Python"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/scikit-image/scikit-image/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """- **Debian/Ubuntu:** ``sudo apt-get install python-skimage``
- **OSX:** ``pip install scikit-image``
- **Anaconda:** ``conda install -c conda-forge scikit-image``
- **Windows:** Download [Windows binaries](http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-image)

Also see [installing ``scikit-image``](INSTALL.rst).

"""^^xsd:string,
        """Install dependencies using:

```
pip install -r requirements.txt
```

Then, install scikit-image using:

```
$ pip install .
```

If you plan to develop the package, you may run it directly from source:

```
$ pip install -e .  #: Do this once to add package to Python path
```

Every time you modify Cython files, also run:

```
$ python setup.py build_ext -i  #: Build binary extensions
```

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/scikit-image/scikit-image/> ;
    sd:keywords "computer-vision"^^xsd:string,
        "image-processing"^^xsd:string,
        "python"^^xsd:string ;
    sd:license "None"^^xsd:anyURI ;
    sd:name "scikit-image/scikit-image"^^xsd:string .

<https://example.org/objects/Software/scikit-learn/scikit-learn/> a sd:Software ;
    sd:author <https://example.org/objects/Person/scikit-learn> ;
    sd:dateCreated "2010-08-17T09:43:38+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T17:55:53+00:00"^^xsd:dateTime ;
    sd:description "scikit-learn: machine learning in Python"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/scikit-learn/scikit-learn/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/scikit-learn/scikit-learn/> ;
    sd:keywords "data-analysis"^^xsd:string,
        "data-science"^^xsd:string,
        "machine-learning"^^xsd:string,
        "python"^^xsd:string,
        "statistics"^^xsd:string ;
    sd:license "None"^^xsd:anyURI ;
    sd:name "scikit-learn/scikit-learn"^^xsd:string .

<https://example.org/objects/Software/sentinelsat/sentinelsat/> a sd:Software ;
    sd:author <https://example.org/objects/Person/sentinelsat> ;
    sd:dateCreated "2015-05-22T20:32:26+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-02T10:20:40+00:00"^^xsd:dateTime ;
    sd:description "Search and download Copernicus Sentinel satellite images"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/sentinelsat/sentinelsat/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """
Install ``sentinelsat`` through pip:

.. code-block:: bash

    pip install sentinelsat

"""^^xsd:string,
        """pip install -e .[dev] 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/sentinelsat/sentinelsat/> ;
    sd:keywords "copernicus"^^xsd:string,
        "esa"^^xsd:string,
        "geographic-data"^^xsd:string,
        "open-data"^^xsd:string,
        "remote-sensing"^^xsd:string,
        "satellite-imagery"^^xsd:string,
        "sentinel"^^xsd:string ;
    sd:license "https://api.github.com/licenses/gpl-3.0"^^xsd:anyURI ;
    sd:name "sentinelsat/sentinelsat"^^xsd:string .

<https://example.org/objects/Software/tensorflow/magenta/> a sd:Software ;
    sd:author <https://example.org/objects/Person/tensorflow> ;
    sd:dateCreated "2016-05-05T20:10:40+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T20:17:13+00:00"^^xsd:dateTime ;
    sd:description "Magenta: Music and Art Generation with Machine Intelligence"^^xsd:string,
        """Take a look at our [colab notebooks](https://magenta.tensorflow.org/demos/colab/) for various models, including one on [getting started](https://colab.research.google.com/notebooks/magenta/hello_magenta/hello_magenta.ipynb).
[Magenta.js](https://github.com/tensorflow/magenta-js) is a also a good resource for models and [demos](https://magenta.tensorflow.org/demos/web/) that run in the browser.
This and more, including [blog posts](https://magenta.tensorflow.org/blog) and [Ableton Live plugins](https://magenta.tensorflow.org/demos/native/), can be found at [https://magenta.tensorflow.org](https://magenta.tensorflow.org).

"""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/tensorflow/magenta/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """If the automated script fails for any reason, or you'd prefer to install by
hand, do the following steps.

Install the Magenta pip package:

```bash
pip install magenta
```

**NOTE**: In order to install the `rtmidi` package that we depend on, you may need to install headers for some sound libraries. On Ubuntu Linux, this command should install the necessary packages:

```bash
sudo apt-get install build-essential libasound2-dev libjack-dev portaudio19-dev
```
On Fedora Linux, use
```bash
sudo dnf group install "C Development Tools and Libraries"
sudo dnf install SAASound-devel jack-audio-connection-kit-devel portaudio-devel
```


The Magenta libraries are now available for use within Python programs and
Jupyter notebooks, and the Magenta scripts are installed in your path!

"""^^xsd:string,
        """If you are running Mac OS X or Ubuntu, you can try using our automated
installation script. Just paste the following command into your terminal.

```bash
curl https://raw.githubusercontent.com/tensorflow/magenta/master/magenta/tools/magenta-install.sh > /tmp/magenta-install.sh
bash /tmp/magenta-install.sh
```

After the script completes, open a new terminal window so the environment
variable changes take effect.

The Magenta libraries are now available for use within Python programs and
Jupyter notebooks, and the Magenta scripts are installed in your path!

Note that you will need to run `source activate magenta` to use Magenta every
time you open a new terminal window.

"""^^xsd:string,
        """Magenta maintains a [pip package](https://pypi.python.org/pypi/magenta) for easy
installation. We recommend using Anaconda to install it, but it can work in any
standard Python environment. We support Python 3 (>= 3.5). These instructions
will assume you are using Anaconda.

"""^^xsd:string,
        """git clone https://github.com/tensorflow/magenta.git 
"""^^xsd:string,
        """pip install -e . 
"""^^xsd:string,
        """pip install . 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/tensorflow/magenta/> ;
    sd:license "https://api.github.com/licenses/apache-2.0"^^xsd:anyURI ;
    sd:name "tensorflow/magenta"^^xsd:string .

<https://example.org/objects/Software/tensorflow/tensorflow/> a sd:Software ;
    sd:author <https://example.org/objects/Person/tensorflow> ;
    sd:dateCreated "2015-11-07T01:19:20+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T22:04:26+00:00"^^xsd:dateTime ;
    sd:description "An Open Source Machine Learning Framework for Everyone"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/tensorflow/tensorflow/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """Linux GPU            |  | PyPI 
"""^^xsd:string,
        """See the [TensorFlow install guide](https://www.tensorflow.org/install) for the
[pip package](https://www.tensorflow.org/install/pip), to
[enable GPU support](https://www.tensorflow.org/install/gpu), use a
[Docker container](https://www.tensorflow.org/install/docker), and
[build from source](https://www.tensorflow.org/install/source).

To install the current release, which includes support for
[CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and
Windows)*:

```
$ pip install tensorflow
```

A smaller CPU-only package is also available:

```
$ pip install tensorflow-cpu
```

To update TensorFlow to the latest version, add `--upgrade` flag to the above
commands.

*Nightly binaries are available for testing using the
[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and
[tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPi.*

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/tensorflow/tensorflow/> ;
    sd:keywords "deep-learning"^^xsd:string,
        "deep-neural-networks"^^xsd:string,
        "distributed"^^xsd:string,
        "machine-learning"^^xsd:string,
        "ml"^^xsd:string,
        "neural-network"^^xsd:string,
        "python"^^xsd:string,
        "tensorflow"^^xsd:string ;
    sd:license "https://api.github.com/licenses/apache-2.0"^^xsd:anyURI ;
    sd:name "tensorflow/tensorflow"^^xsd:string .

<https://example.org/objects/Software/twbs/bootstrap/> a sd:Software ;
    sd:author <https://example.org/objects/Person/twbs> ;
    sd:dateCreated "2011-07-29T21:19:00+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T21:27:21+00:00"^^xsd:dateTime ;
    sd:description """Several quick start options are available:

- [Download the latest release.](https://github.com/twbs/bootstrap/archive/v4.5.0.zip)
- Clone the repo: `git clone https://github.com/twbs/bootstrap.git`
- Install with [npm](https://www.npmjs.com/): `npm install bootstrap`
- Install with [yarn](https://yarnpkg.com/): `yarn add bootstrap@4.5.0`
- Install with [Composer](https://getcomposer.org/): `composer require twbs/bootstrap:4.5.0`
- Install with [NuGet](https://www.nuget.org/): CSS: `Install-Package bootstrap` Sass: `Install-Package bootstrap.sass`

Read the [Getting started page](https://getbootstrap.com/docs/4.5/getting-started/introduction/) for information on the framework contents, templates and examples, and more.


"""^^xsd:string,
        "The most popular HTML, CSS, and JavaScript framework for developing responsive, mobile first projects on the web."^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/twbs/bootstrap/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/twbs/bootstrap/> ;
    sd:keywords "bootstrap"^^xsd:string,
        "css"^^xsd:string,
        "css-framework"^^xsd:string,
        "html"^^xsd:string,
        "javascript"^^xsd:string,
        "sass"^^xsd:string,
        "scss"^^xsd:string ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "twbs/bootstrap"^^xsd:string .

<https://example.org/objects/Software/ungarj/tilematrix/> a sd:Software ;
    sd:author <https://example.org/objects/Person/ungarj> ;
    sd:dateCreated "2015-08-26T15:06:39+00:00"^^xsd:dateTime ;
    sd:dateModified "2019-08-29T14:40:17+00:00"^^xsd:dateTime ;
    sd:description "helps handling tile pyramids"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/ungarj/tilematrix/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """Use pip to install the latest stable version: 
"""^^xsd:string,
        """pip install -r requirements.txt 
python setup.py install 
"""^^xsd:string,
        """pip install tilematrix 
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/ungarj/tilematrix/> ;
    sd:keywords "geospatial"^^xsd:string,
        "mercator-projection"^^xsd:string,
        "pyramid"^^xsd:string,
        "tile"^^xsd:string,
        "tilesets"^^xsd:string,
        "web-mapping"^^xsd:string,
        "wgs84"^^xsd:string ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "ungarj/tilematrix"^^xsd:string .

<https://example.org/objects/Software/vuejs/vue/> a sd:Software ;
    sd:author <https://example.org/objects/Person/vuejs> ;
    sd:dateCreated "2013-07-29T03:24:51+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-06-03T22:00:26+00:00"^^xsd:dateTime ;
    sd:description """Vue (pronounced `/vjuː/`, like view) is a **progressive framework** for building user interfaces. It is designed from the ground up to be incrementally adoptable, and can easily scale between a library and a framework depending on different use cases. It consists of an approachable core library that focuses on the view layer only, and an ecosystem of supporting libraries that helps you tackle complexity in large Single-Page Applications.

"""^^xsd:string,
        "🖖 Vue.js is a progressive, incrementally-adoptable JavaScript framework for building UI on the web."^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/vuejs/vue/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/vuejs/vue/> ;
    sd:keywords "framework"^^xsd:string,
        "frontend"^^xsd:string,
        "javascript"^^xsd:string,
        "vue"^^xsd:string ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "vuejs/vue"^^xsd:string .

<https://example.org/objects/Software/whimian/pyGeoPressure/> a sd:Software ;
    sd:author <https://example.org/objects/Person/whimian> ;
    sd:citation """  year = {2018}, 
"""^^xsd:string,
        """@article{yu2018pygeopressure,
  title = {{PyGeoPressure}: {Geopressure} {Prediction} in {Python}},
  author = {Yu, Hao},
  journal = {Journal of Open Source Software},
  volume = {3},
  pages = {922}
  number = {30},
  year = {2018},
  doi = {10.21105/joss.00992},
}"""^^xsd:string ;
    sd:dateCreated "2015-05-26T13:59:34+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-04-20T06:52:55+00:00"^^xsd:dateTime ;
    sd:description "Pore pressure prediction using seismic velocity and well log data"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/whimian/pyGeoPressure/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """`pyGeoPressure` is on `PyPI`:

```bash
pip install pygeopressure
```

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/whimian/pyGeoPressure/> ;
    sd:keywords "formation-pressure"^^xsd:string,
        "geomechancis"^^xsd:string,
        "geophysics"^^xsd:string,
        "geopressure"^^xsd:string,
        "pore-pressure-prediction"^^xsd:string ;
    sd:license "https://api.github.com/licenses/mit"^^xsd:anyURI ;
    sd:name "whimian/pyGeoPressure"^^xsd:string .

<https://example.org/objects/Software/wuhuikai/DeepGuidedFilter/> a sd:Software ;
    sd:author <https://example.org/objects/Person/wuhuikai> ;
    sd:citation """@inproceedings{wu2017fast,
  title     = {Fast End-to-End Trainable Guided Filter},
  author    = {Wu, Huikai and Zheng, Shuai and Zhang, Junge and Huang, Kaiqi},
  booktitle = {CVPR},
  year = {2018}
}"""^^xsd:string,
        """```
@inproceedings{wu2017fast,
  title     = {Fast End-to-End Trainable Guided Filter},
  author    = {Wu, Huikai and Zheng, Shuai and Zhang, Junge and Huang, Kaiqi},
  booktitle = {CVPR},
  year = {2018}
}
```"""^^xsd:string ;
    sd:dateCreated "2018-02-26T08:14:20+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-29T06:56:22+00:00"^^xsd:dateTime ;
    sd:description """![](images/results.jpg)

**DeepGuidedFilter** is the author's implementation of:

**Fast End-to-End Trainable Guided Filter**     
Huikai Wu, Shuai Zheng, Junge Zhang, Kaiqi Huang    
CVPR 2018

With our method, FCNs can run **10-100** times faster w/o performance drop.

Contact: Hui-Kai Wu (huikaiwu@icloud.com)

"""^^xsd:string,
        """* [Image Processing](ImageProcessing/DeepGuidedFilteringNetwork)
* [Semantic Segmentation with Deeplab-Resnet](ComputerVision/Deeplab-Resnet)
* [Saliency Detection with DSS](ComputerVision/Saliency_DSS)
* [Monocular Depth Estimation](ComputerVision/MonoDepth)

"""^^xsd:string,
        "Official Implementation of Fast End-to-End Trainable Guided Filter, CVPR 2018"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/wuhuikai/DeepGuidedFilter/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """* PyTorch Version
    ```sh
    pip install guided-filter-pytorch
    ```
* Tensorflow Version
    ```sh
    pip install guided-filter-tf
    ```
"""^^xsd:string,
        """1. Download source code from GitHub.
    ```sh
    git clone https://github.com/wuhuikai/DeepGuidedFilter
    
    cd DeepGuidedFilter && git checkout release
    ```
2. Install dependencies.
    ```sh
    conda install opencv=3.4
    conda install pytorch=1.1 torchvision=0.2 cudatoolkit=9.0 -c pytorch
    
    pip install -r requirements.txt 
    ```
3. (**Optional**) Install dependencies for MonoDepth.
    ```sh
    cd ComputerVision/MonoDepth
    
    pip install -r requirements.txt
    ```
"""^^xsd:string,
        """[[Project]](http://wuhuikai.me/DeepGuidedFilterProject)    [[Paper]](http://wuhuikai.me/DeepGuidedFilterProject/deep_guided_filter.pdf)    [[arXiv]](https://arxiv.org/abs/1803.05619)    [[Demo]](http://wuhuikai.me/DeepGuidedFilterProject#demo)    [[Home]](http://wuhuikai.me)
  
Official implementation of **Fast End-to-End Trainable Guided Filter**.     
**Faster**, **Better** and **Lighter** for pixel-wise image prediction.

"""^^xsd:string,
        """```sh
git checkout master

conda install opencv=3.4
conda install pytorch=1.1 torchvision=0.2 cudatoolkit=9.0 -c pytorch

pip uninstall Pillow
pip install -r requirements.txt

#: (Optional) For MonoDepth
pip install -r ComputerVision/MonoDepth/requirements.txt 
```
"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/wuhuikai/DeepGuidedFilter/> ;
    sd:name "wuhuikai/DeepGuidedFilter"^^xsd:string .

<https://example.org/objects/Software/yuhuayc/da-faster-rcnn/> a sd:Software ;
    sd:author <https://example.org/objects/Person/yuhuayc> ;
    sd:citation """  booktitle = {Computer Vision and Pattern Recognition (CVPR)}, 
  year={2018} 
"""^^xsd:string,
        """@inproceedings{chen2018domain,
  title={Domain Adaptive Faster R-CNN for Object Detection in the Wild},
  author={Chen, Yuhua and Li, Wen and Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
  booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  year={2018}
}"""^^xsd:string,
        """The implementation is built on the python implementation of Faster RCNN [rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn)

"""^^xsd:string ;
    sd:dateCreated "2018-05-15T12:31:23+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-30T10:02:47+00:00"^^xsd:dateTime ;
    sd:description "An implementation of our CVPR 2018 work 'Domain Adaptive Faster R-CNN for Object Detection in the Wild'"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/yuhuayc/da-faster-rcnn/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/yuhuayc/da-faster-rcnn/> ;
    sd:license "None"^^xsd:anyURI ;
    sd:name "yuhuayc/da-faster-rcnn"^^xsd:string .

<https://example.org/objects/Software/yulunzhang/RDN/> a sd:Software ;
    sd:author <https://example.org/objects/Person/yulunzhang> ;
    sd:citation """@InProceedings{Lim_2017_CVPR_Workshops,
  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},
  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  month = {July},
  year = {2017}
}"""^^xsd:string,
        """@article{zhang2018rdnir,
    title={Residual Dense Network for Image Restoration},
    author={Zhang, Yulun and Tian, Yapeng and Kong, Yu and Zhong, Bineng and Fu, Yun},
    booktitle={arXiv},
    year={2018}
}"""^^xsd:string,
        """@inproceedings{zhang2018residual,
    title={Residual Dense Network for Image Super-Resolution},
    author={Zhang, Yulun and Tian, Yapeng and Kong, Yu and Zhong, Bineng and Fu, Yun},
    booktitle={CVPR},
    year={2018}
}"""^^xsd:string,
        """If you find the code helpful in your resarch or work, please cite the following papers.
```
@InProceedings{Lim_2017_CVPR_Workshops,
  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},
  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  month = {July},
  year = {2017}
}

@inproceedings{zhang2018residual,
    title={Residual Dense Network for Image Super-Resolution},
    author={Zhang, Yulun and Tian, Yapeng and Kong, Yu and Zhong, Bineng and Fu, Yun},
    booktitle={CVPR},
    year={2018}
}

@article{zhang2018rdnir,
    title={Residual Dense Network for Image Restoration},
    author={Zhang, Yulun and Tian, Yapeng and Kong, Yu and Zhong, Bineng and Fu, Yun},
    booktitle={arXiv},
    year={2018}
}

```
"""^^xsd:string ;
    sd:dateCreated "2018-03-30T23:56:55+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-29T00:42:32+00:00"^^xsd:dateTime ;
    sd:description """1. (optional) Download models for our paper and place them in '/RDN_TrainCode/experiment/model'.

    All the models can be downloaded from [Dropbox](https://www.dropbox.com/sh/ngcvqdas167gol2/AAAdJe9w6s2fpo_KEGZe7d4Ra?dl=0) or [Baidu](https://pan.baidu.com/s/116FAzKnaJnAdxY_B6ENp_A).

2. Cd to 'RDN_TrainCode/code', run the following scripts to train models.

    **You can use scripts in file 'TrainRDN_scripts' to train models for our paper.**

    ```bash
    #: BI, scale 2, 3, 4
    #: BIX2F64D18C6G64P48, input=48x48, output=96x96
    th main.lua -scale 2 -netType RDN -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true

    #: BIX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX2.t7
    th main.lua -scale 3 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true  -preTrained ../experiment/model/RDN_BIX2.t7

    #: BIX4F64D18C6G64P32, input=32x32, output=128x128, fine-tune on RDN_BIX2.t7
    th main.lua -scale 4 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 128 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true -nEpochs 1000 -preTrained ../experiment/model/RDN_BIX2.t7 

    #: BD, scale 3
    #: BDX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX3.t7
    th main.lua -scale 3 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BD -splitBatch 4 -trainOnly true -nEpochs 200 -preTrained ../experiment/model/RDN_BIX3.t7

    #: DN, scale 3
    #: DNX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX3.t7
    th main.lua -scale 3 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel DN -splitBatch 4 -trainOnly true  -nEpochs 200 -preTrained ../experiment/model/RDN_BIX3.t7
    ```
    Only RDN_BIX2.t7 was trained using 48x48 input patches. All other models were trained using 32x32 input patches in order to save training time.
    However, smaller input patch size in training would lower the performance to some degree. We also set '-trainOnly true' to save GPU memory.
"""^^xsd:string,
        """1. Download models for our paper and place them in '/RDN_TestCode/model'.

    All the models can be downloaded from [Dropbox](https://www.dropbox.com/sh/ngcvqdas167gol2/AAAdJe9w6s2fpo_KEGZe7d4Ra?dl=0) or [Baidu](https://pan.baidu.com/s/116FAzKnaJnAdxY_B6ENp_A).

2. Run 'TestRDN.lua'

    **You can use scripts in file 'TestRDN_scripts' to produce results for our paper.**

    ```bash
    #: No self-ensemble: RDN
    #: BI degradation model, X2, X3, X4
    th TestRDN.lua -model RDN_BIX2 -degradation BI -scale 2 -selfEnsemble false -dataset Set5
    th TestRDN.lua -model RDN_BIX3 -degradation BI -scale 3 -selfEnsemble false -dataset Set5
    th TestRDN.lua -model RDN_BIX4 -degradation BI -scale 4 -selfEnsemble false -dataset Set5
    #: BD degradation model, X3
    th TestRDN.lua -model RDN_BDX3 -degradation BD -scale 3 -selfEnsemble false -dataset Set5
    #: DN degradation model, X3
    th TestRDN.lua -model RDN_DNX3 -degradation DN -scale 3 -selfEnsemble false -dataset Set5


    #: With self-ensemble: RDN+
    #: BI degradation model, X2, X3, X4
    th TestRDN.lua -model RDN_BIX2 -degradation BI -scale 2 -selfEnsemble true -dataset Set5
    th TestRDN.lua -model RDN_BIX3 -degradation BI -scale 3 -selfEnsemble true -dataset Set5
    th TestRDN.lua -model RDN_BIX4 -degradation BI -scale 4 -selfEnsemble true -dataset Set5
    #: BD degradation model, X3
    th TestRDN.lua -model RDN_BDX3 -degradation BD -scale 3 -selfEnsemble true -dataset Set5
    #: DN degradation model, X3
    th TestRDN.lua -model RDN_DNX3 -degradation DN -scale 3 -selfEnsemble true -dataset Set5
    ```

"""^^xsd:string,
        """A very deep convolutional neural network (CNN) has recently achieved great success for image super-resolution (SR) and offered hierarchical features as well. However, most deep CNN based SR models do not make full use of the hierarchical features from the original low-resolution (LR) images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in image SR. We fully exploit the hierarchical features from all the convolutional layers. Speciﬁcally, we propose residual dense block (RDB) to extract abundant local features via dense connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is then used to adaptively learn more effective features from preceding and current local features and stabilizes the training of wider network. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Experiments on benchmark datasets with different degradation models show that our RDN achieves favorable performance against state-of-the-art methods.

![RDB](/Figs/RDB.png)
Figure 1. Residual dense block (RDB) architecture.
![RDN](/Figs/RDN.png)
Figure 2. The architecture of our proposed residual dense network (RDN).

"""^^xsd:string,
        "Torch code for our CVPR 2018 paper \"Residual Dense Network for Image Super-Resolution\" (Spotlight)"^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/yulunzhang/RDN/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasInstallInstructions """1. Download DIV2K training data (800 training + 100 validtion images) from [DIV2K dataset](https://data.vision.ee.ethz.ch/cvl/DIV2K/) or [SNU_CVLab](https://cv.snu.ac.kr/research/EDSR/DIV2K.tar).

2. Place all the HR images in 'Prepare_TrainData/DIV2K/DIV2K_HR'.

3. Run 'Prepare_TrainData_HR_LR_BI/BD/DN.m' in matlab to generate LR images for BI, BD, and DN models respectively.

4. Run 'th png_to_t7.lua' to convert each .png image to .t7 file in new folder 'DIV2K_decoded'.

5. Specify the path of 'DIV2K_decoded' to '-datadir' in 'RDN_TrainCode/code/opts.lua'.

For more informaiton, please refer to [EDSR(Torch)](https://github.com/LimBee/NTIRE2017).

"""^^xsd:string ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/yulunzhang/RDN/> ;
    sd:keywords "super-resolution"^^xsd:string ;
    sd:name "yulunzhang/RDN"^^xsd:string .

<https://example.org/objects/Software/zhiqiangdon/CU-Net/> a sd:Software ;
    sd:author <https://example.org/objects/Person/zhiqiangdon> ;
    sd:citation """@inproceedings{tang2018cu,
  title={CU-Net: Coupled U-Nets},
  author={Tang, Zhiqiang and Peng, Xi and Geng, Shijie and Zhu, Yizhe and Metaxas, Dimitris},
  booktitle={BMVC},
  year={2018}
}"""^^xsd:string,
        """@inproceedings{tang2018quantized,
  title={Quantized densely connected U-Nets for efficient landmark localization},
  author={Tang, Zhiqiang and Peng, Xi and Geng, Shijie and Wu, Lingfei and Zhang, Shaoting and Metaxas, Dimitris},
  booktitle={ECCV},
  year={2018}
}"""^^xsd:string,
        """If you find this code useful in your research, please consider citing:

```
@inproceedings{tang2018quantized,
  title={Quantized densely connected U-Nets for efficient landmark localization},
  author={Tang, Zhiqiang and Peng, Xi and Geng, Shijie and Wu, Lingfei and Zhang, Shaoting and Metaxas, Dimitris},
  booktitle={ECCV},
  year={2018}
}
@inproceedings{tang2018cu,
  title={CU-Net: Coupled U-Nets},
  author={Tang, Zhiqiang and Peng, Xi and Geng, Shijie and Zhu, Yizhe and Metaxas, Dimitris},
  booktitle={BMVC},
  year={2018}
}
```

"""^^xsd:string ;
    sd:dateCreated "2018-07-25T21:07:30+00:00"^^xsd:dateTime ;
    sd:dateModified "2020-05-20T16:14:19+00:00"^^xsd:dateTime ;
    sd:description "Code for \"Quantized Densely Connected U-Nets for Efficient Landmark Localization\" (ECCV 2018) and \"CU-Net: Coupled U-Nets\" (BMVC 2018 oral)"^^xsd:string,
        """The follwoing figure gives an illustration of naive dense U-Net, stacked U-Nets and coupled U-Nets (CU-Net). The naive dense U-Net and stacked U-Nets have shortcut connections only inside each U-Net. In contrast, the coupled U-Nets also have connections for semantic blocks across U-Nets. The CU-Net is a hybrid of naive dense U-Net and stacked U-Net, integrating the merits of both dense connectivity, intermediate supervisions and multi-stage top-down and bottom-up refinement. The resulted CU-Net could save ~70% parameters of the previous stacked U-Nets but with comparable accuracy.


If we couple each U-Net pair in multiple U-Nets, the coupling connections would have quadratic growth with respect to the U-Net number. To make the model more parameter efficient, we propose the order-K coupling to trim off the long-distance coupling connections.

For simplicity, each dot represents one U-Net. The red and blue lines are the shortcut connections of inside semantic blocks and outside inputs. Order-0 connectivity (Top) strings U-Nets together only by their inputs and outputs, i.e. stacked U-Nets. Order-1 connectivity (Middle) has shortcut connections for adjacent U-Nets. Similarly, order-2 connectivity (Bottom) has shortcut connections for 3 nearby U-Nets.

"""^^xsd:string ;
    sd:hasDownloadUrl "https://api.github.com/repos/zhiqiangdon/CU-Net/{archive_format}{/ref}"^^xsd:anyURI ;
    sd:hasSourceCode <https://example.org/objects/SoftwareSource/zhiqiangdon/CU-Net/> ;
    sd:keywords "face-align"^^xsd:string,
        "human-pose-estimation"^^xsd:string,
        "quantized-neural-network"^^xsd:string,
        "quantized-training"^^xsd:string,
        "u-net"^^xsd:string ;
    sd:license "https://api.github.com/licenses/apache-2.0"^^xsd:anyURI ;
    sd:name "zhiqiangdon/CU-Net"^^xsd:string .

<https://example.org/objects/Person/JimmySuen> a schema:Person ;
    schema:additionalName "JimmySuen"^^schema:Text .

<https://example.org/objects/Person/JuliaGeo> a schema:Person ;
    schema:additionalName "JuliaGeo"^^schema:Text .

<https://example.org/objects/Person/LMescheder> a schema:Person ;
    schema:additionalName "LMescheder"^^schema:Text .

<https://example.org/objects/Person/NSGeophysics> a schema:Person ;
    schema:additionalName "NSGeophysics"^^schema:Text .

<https://example.org/objects/Person/NVIDIA> a schema:Person ;
    schema:additionalName "NVIDIA"^^schema:Text .

<https://example.org/objects/Person/OpenGeoscience> a schema:Person ;
    schema:additionalName "OpenGeoscience"^^schema:Text .

<https://example.org/objects/Person/XiaLiPKU> a schema:Person ;
    schema:additionalName "XiaLiPKU"^^schema:Text .

<https://example.org/objects/Person/ZhouYanzhao> a schema:Person ;
    schema:additionalName "ZhouYanzhao"^^schema:Text .

<https://example.org/objects/Person/agile-geoscience> a schema:Person ;
    schema:additionalName "agile-geoscience"^^schema:Text .

<https://example.org/objects/Person/akaszynski> a schema:Person ;
    schema:additionalName "akaszynski"^^schema:Text .

<https://example.org/objects/Person/albertpumarola> a schema:Person ;
    schema:additionalName "albertpumarola"^^schema:Text .

<https://example.org/objects/Person/cgre-aachen> a schema:Person ;
    schema:additionalName "cgre-aachen"^^schema:Text .

<https://example.org/objects/Person/cltk> a schema:Person ;
    schema:additionalName "cltk"^^schema:Text .

<https://example.org/objects/Person/d3> a schema:Person ;
    schema:additionalName "d3"^^schema:Text .

<https://example.org/objects/Person/driftingtides> a schema:Person ;
    schema:additionalName "driftingtides"^^schema:Text .

<https://example.org/objects/Person/driving-behavior> a schema:Person ;
    schema:additionalName "driving-behavior"^^schema:Text .

<https://example.org/objects/Person/empymod> a schema:Person ;
    schema:additionalName "empymod"^^schema:Text .

<https://example.org/objects/Person/facebook> a schema:Person ;
    schema:additionalName "facebook"^^schema:Text .

<https://example.org/objects/Person/foolwood> a schema:Person ;
    schema:additionalName "foolwood"^^schema:Text .

<https://example.org/objects/Person/geo-data> a schema:Person ;
    schema:additionalName "geo-data"^^schema:Text .

<https://example.org/objects/Person/geopandas> a schema:Person ;
    schema:additionalName "geopandas"^^schema:Text .

<https://example.org/objects/Person/gitbucket> a schema:Person ;
    schema:additionalName "gitbucket"^^schema:Text .

<https://example.org/objects/Person/google> a schema:Person ;
    schema:additionalName "google"^^schema:Text .

<https://example.org/objects/Person/gprMax> a schema:Person ;
    schema:additionalName "gprMax"^^schema:Text .

<https://example.org/objects/Person/haoliangyu> a schema:Person ;
    schema:additionalName "haoliangyu"^^schema:Text .

<https://example.org/objects/Person/harismuneer> a schema:Person ;
    schema:additionalName "harismuneer"^^schema:Text .

<https://example.org/objects/Person/hiroharu-kato> a schema:Person ;
    schema:additionalName "hiroharu-kato"^^schema:Text .

<https://example.org/objects/Person/iannesbitt> a schema:Person ;
    schema:additionalName "iannesbitt"^^schema:Text .

<https://example.org/objects/Person/imfunniee> a schema:Person ;
    schema:additionalName "imfunniee"^^schema:Text .

<https://example.org/objects/Person/joferkington> a schema:Person ;
    schema:additionalName "joferkington"^^schema:Text .

<https://example.org/objects/Person/jupyter-widgets> a schema:Person ;
    schema:additionalName "jupyter-widgets"^^schema:Text .

<https://example.org/objects/Person/jwass> a schema:Person ;
    schema:additionalName "jwass"^^schema:Text .

<https://example.org/objects/Person/kinverarity1> a schema:Person ;
    schema:additionalName "kinverarity1"^^schema:Text .

<https://example.org/objects/Person/kosmtik> a schema:Person ;
    schema:additionalName "kosmtik"^^schema:Text .

<https://example.org/objects/Person/mbloch> a schema:Person ;
    schema:additionalName "mbloch"^^schema:Text .

<https://example.org/objects/Person/msracver> a schema:Person ;
    schema:additionalName "msracver"^^schema:Text .

<https://example.org/objects/Person/nextflow-io> a schema:Person ;
    schema:additionalName "nextflow-io"^^schema:Text .

<https://example.org/objects/Person/nypl-spacetime> a schema:Person ;
    schema:additionalName "nypl-spacetime"^^schema:Text .

<https://example.org/objects/Person/odoe> a schema:Person ;
    schema:additionalName "odoe"^^schema:Text .

<https://example.org/objects/Person/ondrolexa> a schema:Person ;
    schema:additionalName "ondrolexa"^^schema:Text .

<https://example.org/objects/Person/phoenix104104> a schema:Person ;
    schema:additionalName "phoenix104104"^^schema:Text .

<https://example.org/objects/Person/phuang17> a schema:Person ;
    schema:additionalName "phuang17"^^schema:Text .

<https://example.org/objects/Person/puppeteer> a schema:Person ;
    schema:additionalName "puppeteer"^^schema:Text .

<https://example.org/objects/Person/pyro-ppl> a schema:Person ;
    schema:additionalName "pyro-ppl"^^schema:Text .

<https://example.org/objects/Person/pysal> a schema:Person ;
    schema:additionalName "pysal"^^schema:Text .

<https://example.org/objects/Person/reduxjs> a schema:Person ;
    schema:additionalName "reduxjs"^^schema:Text .

<https://example.org/objects/Person/rowanz> a schema:Person ;
    schema:additionalName "rowanz"^^schema:Text .

<https://example.org/objects/Person/salihkaragoz> a schema:Person ;
    schema:additionalName "salihkaragoz"^^schema:Text .

<https://example.org/objects/Person/scikit-image> a schema:Person ;
    schema:additionalName "scikit-image"^^schema:Text .

<https://example.org/objects/Person/scikit-learn> a schema:Person ;
    schema:additionalName "scikit-learn"^^schema:Text .

<https://example.org/objects/Person/sentinelsat> a schema:Person ;
    schema:additionalName "sentinelsat"^^schema:Text .

<https://example.org/objects/Person/twbs> a schema:Person ;
    schema:additionalName "twbs"^^schema:Text .

<https://example.org/objects/Person/ungarj> a schema:Person ;
    schema:additionalName "ungarj"^^schema:Text .

<https://example.org/objects/Person/vuejs> a schema:Person ;
    schema:additionalName "vuejs"^^schema:Text .

<https://example.org/objects/Person/whimian> a schema:Person ;
    schema:additionalName "whimian"^^schema:Text .

<https://example.org/objects/Person/wuhuikai> a schema:Person ;
    schema:additionalName "wuhuikai"^^schema:Text .

<https://example.org/objects/Person/yuhuayc> a schema:Person ;
    schema:additionalName "yuhuayc"^^schema:Text .

<https://example.org/objects/Person/yulunzhang> a schema:Person ;
    schema:additionalName "yulunzhang"^^schema:Text .

<https://example.org/objects/Person/zhiqiangdon> a schema:Person ;
    schema:additionalName "zhiqiangdon"^^schema:Text .

<https://example.org/objects/SoftwareSource/JimmySuen/integral-human-pose/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/JimmySuen/integral-human-pose"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/JuliaGeo/LibGEOS.jl/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/JuliaGeo/LibGEOS.jl"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/LMescheder/GAN_stability/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/LMescheder/GAN_stability"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/NSGeophysics/GPRPy/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/NSGeophysics/GPRPy"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/NVIDIA/vid2vid/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/NVIDIA/vid2vid"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/OpenGeoVis/PVGeo/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/OpenGeoVis/PVGeo"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/OpenGeoVis/omfvista/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/OpenGeoVis/omfvista"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/OpenGeoscience/geonotebook/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/OpenGeoscience/geonotebook"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/Toblerity/Fiona/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/Toblerity/Fiona"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/Toblerity/Shapely/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/Toblerity/Shapely"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/XiaLiPKU/RESCAN/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/XiaLiPKU/RESCAN"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/ZhouYanzhao/PRM/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/ZhouYanzhao/PRM"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/agile-geoscience/striplog/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/agile-geoscience/striplog"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/akaszynski/pyansys/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/akaszynski/pyansys"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/albertpumarola/GANimation/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/albertpumarola/GANimation"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/cgre-aachen/gempy/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/cgre-aachen/gempy"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/cltk/cltk/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/cltk/cltk"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/d3/d3/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/d3/d3"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/driftingtides/hyvr/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/driftingtides/hyvr"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/driving-behavior/DBNet/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/driving-behavior/DBNet"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/empymod/empymod/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/empymod/empymod"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/equinor/pylops/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/equinor/pylops"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/equinor/segyio/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/equinor/segyio"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/facebook/react/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/facebook/react"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/facebookresearch/DensePose/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/facebookresearch/DensePose"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/facebookresearch/Detectron/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/facebookresearch/Detectron"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/facebookresearch/ResNeXt/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/facebookresearch/ResNeXt"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/facebookresearch/pyrobot/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/facebookresearch/pyrobot"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/foolwood/DaSiamRPN/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/foolwood/DaSiamRPN"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/geo-data/gdal-docker/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/geo-data/gdal-docker"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/geopandas/geopandas/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/geopandas/geopandas"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/gitbucket/gitbucket/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/gitbucket/gitbucket"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/google/sg2im/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/google/sg2im"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/gprMax/gprMax/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/gprMax/gprMax"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/haoliangyu/node-qa-masker/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/haoliangyu/node-qa-masker"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/harismuneer/Ultimate-Facebook-Scraper/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/harismuneer/Ultimate-Facebook-Scraper"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/hezhangsprinter/DCPDN/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/hezhangsprinter/DCPDN"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/hezhangsprinter/DID-MDN/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/hezhangsprinter/DID-MDN"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/hiroharu-kato/neural_renderer/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/hiroharu-kato/neural_renderer"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/iannesbitt/readgssi/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/iannesbitt/readgssi"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/imfunniee/gitfolio/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/imfunniee/gitfolio"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/joferkington/mplstereonet/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/joferkington/mplstereonet"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/jupyter-widgets/ipyleaflet/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/jupyter-widgets/ipyleaflet"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/jwass/mplleaflet/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/jwass/mplleaflet"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/kinverarity1/lasio/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/kinverarity1/lasio"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/kosmtik/kosmtik/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/kosmtik/kosmtik"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/mapbox/geojson-vt/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/mapbox/geojson-vt"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/mapbox/rasterio/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/mapbox/rasterio"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/mapbox/tilelive-mapnik/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/mapbox/tilelive-mapnik"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/mapbox/tippecanoe/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/mapbox/tippecanoe"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/mbloch/mapshaper/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/mbloch/mapshaper"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/msracver/Flow-Guided-Feature-Aggregation/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/msracver/Flow-Guided-Feature-Aggregation"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/nextflow-io/nextflow/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/nextflow-io/nextflow"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/nypl-spacetime/map-vectorizer/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/nypl-spacetime/map-vectorizer"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/odoe/generator-arcgis-js-app/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/odoe/generator-arcgis-js-app"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/ondrolexa/apsg/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/ondrolexa/apsg"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/phoenix104104/LapSRN/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/phoenix104104/LapSRN"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/phuang17/DeepMVS/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/phuang17/DeepMVS"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/puppeteer/puppeteer/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/puppeteer/puppeteer"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/pyro-ppl/pyro/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/pyro-ppl/pyro"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/pysal/pysal/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/pysal/pysal"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/pyvista/pymeshfix/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/pyvista/pymeshfix"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/pyvista/pyvista/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/pyvista/pyvista"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/pyvista/tetgen/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/pyvista/tetgen"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/reduxjs/react-redux/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/reduxjs/react-redux"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/rowanz/neural-motifs/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/rowanz/neural-motifs"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/salihkaragoz/pose-residual-network-pytorch/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/salihkaragoz/pose-residual-network-pytorch"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/scikit-image/scikit-image/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/scikit-image/scikit-image"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/scikit-learn/scikit-learn/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/scikit-learn/scikit-learn"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/sentinelsat/sentinelsat/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/sentinelsat/sentinelsat"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/tensorflow/magenta/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/tensorflow/magenta"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/tensorflow/tensorflow/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/tensorflow/tensorflow"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/twbs/bootstrap/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/twbs/bootstrap"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/ungarj/tilematrix/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/ungarj/tilematrix"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/vuejs/vue/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/vuejs/vue"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/whimian/pyGeoPressure/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/whimian/pyGeoPressure"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/wuhuikai/DeepGuidedFilter/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/wuhuikai/DeepGuidedFilter"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/yuhuayc/da-faster-rcnn/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/yuhuayc/da-faster-rcnn"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/yulunzhang/RDN/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/yulunzhang/RDN"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/SoftwareSource/zhiqiangdon/CU-Net/> a sd:SoftwareSource ;
    sd:codeRepository "https://github.com/zhiqiangdon/CU-Net"^^xsd:anyURI ;
    sd:programmingLanguage "documentation_url"^^xsd:string,
        "message"^^xsd:string .

<https://example.org/objects/Person/OpenGeoVis> a schema:Person ;
    schema:additionalName "OpenGeoVis"^^schema:Text .

<https://example.org/objects/Person/Toblerity> a schema:Person ;
    schema:additionalName "Toblerity"^^schema:Text .

<https://example.org/objects/Person/equinor> a schema:Person ;
    schema:additionalName "equinor"^^schema:Text .

<https://example.org/objects/Person/hezhangsprinter> a schema:Person ;
    schema:additionalName "hezhangsprinter"^^schema:Text .

<https://example.org/objects/Person/tensorflow> a schema:Person ;
    schema:additionalName "tensorflow"^^schema:Text .

<https://example.org/objects/Person/pyvista> a schema:Person ;
    schema:additionalName "pyvista"^^schema:Text .

<https://example.org/objects/Person/facebookresearch> a schema:Person ;
    schema:additionalName "facebookresearch"^^schema:Text .

<https://example.org/objects/Person/mapbox> a schema:Person ;
    schema:additionalName "mapbox"^^schema:Text .

